{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个代码文件是把一个预训练模型ERNIE1.0构成的神经网络 利用in-batch-negative方法进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from scipy.special import softmax\n",
    "from scipy.special import expit\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle import inference\n",
    "import paddlenlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset, MapDataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "from paddlenlp.utils.downloader import get_path_from_url\n",
    "from visualdl import LogWriter\n",
    "from data import convert_pairwise_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verify PaddlePaddle program ... \n",
      "PaddlePaddle works well on 1 GPU.\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n"
     ]
    }
   ],
   "source": [
    "paddle.utils.run_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle.set_device(\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_pair(data_path): \n",
    "  with open(data_path, 'r', encoding='utf-8') as f: \n",
    "        for line in f: \n",
    "            data = line.rstrip().split(\"\\t\") \n",
    "            if len(data) != 2: \n",
    "                continue\n",
    "            yield {'text_a': data[0], 'text_b': data[1]} \n",
    "                                                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set_path='recall_dataset/train.csv' \n",
    "train_ds = load_dataset(read_text_pair, data_path=train_set_path, lazy=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'paddlenlp.datasets.dataset.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_a': '从《唐律疏义》看唐代封爵贵族的法律特权', 'text_b': '从《唐律疏义》看唐代封爵贵族的法律特权《唐律疏义》,封爵贵族,法律特权'}\n",
      "{'text_a': '宁夏社区图书馆服务体系布局现状分析', 'text_b': '宁夏社区图书馆服务体系布局现状分析社区图书馆,社区图书馆服务,社区图书馆服务体系'}\n",
      "{'text_a': '人口老龄化对京津冀经济', 'text_b': '京津冀人口老龄化对区域经济增长的影响京津冀,人口老龄化,区域经济增长,固定效应模型'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面开始构造训练数据的加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:10:15,430] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie/vocab.txt and saved to C:\\Users\\LJX\\.paddlenlp\\models\\ernie-1.0\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:10:15,973] [    INFO]\u001b[0m - Downloading vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie/vocab.txt\u001b[0m\n",
      "100%|██████████| 89.5k/89.5k [00:00<00:00, 98.2kB/s]\n",
      "\u001b[32m[2025-02-06 22:10:17,458] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\LJX\\.paddlenlp\\models\\ernie-1.0\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:10:17,461] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\LJX\\.paddlenlp\\models\\ernie-1.0\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME=\"ernie-1.0\"\n",
    "\n",
    "tokenizer = paddlenlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=512): \n",
    "    result = []\n",
    "    \n",
    "    for key, text in example.items(): \n",
    "        encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\n",
    "        input_ids = encoded_inputs[\"input_ids\"] \n",
    "        token_type_ids = encoded_inputs[\"token_type_ids\"] \n",
    "        result += [input_ids, token_type_ids]\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从《唐律疏义》看唐代封爵贵族的法律特权\n",
      "{'input_ids': [1, 158, 56, 867, 646, 1500, 393, 55, 335, 867, 140, 898, 2153, 864, 495, 5, 72, 646, 169, 438, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "[1, 158, 56, 867, 646, 1500, 393, 55, 335, 867, 140, 898, 2153, 864, 495, 5, 72, 646, 169, 438, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "------------------------------------\n",
      "从《唐律疏义》看唐代封爵贵族的法律特权《唐律疏义》,封爵贵族,法律特权\n",
      "{'input_ids': [1, 158, 56, 867, 646, 1500, 393, 55, 335, 867, 140, 898, 2153, 864, 495, 5, 72, 646, 169, 438, 56, 867, 646, 1500, 393, 55, 30, 898, 2153, 864, 495, 30, 72, 646, 169, 438, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "[1, 158, 56, 867, 646, 1500, 393, 55, 335, 867, 140, 898, 2153, 864, 495, 5, 72, 646, 169, 438, 56, 867, 646, 1500, 393, 55, 30, 898, 2153, 864, 495, 30, 72, 646, 169, 438, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "------------------------------------\n",
      "[[1, 158, 56, 867, 646, 1500, 393, 55, 335, 867, 140, 898, 2153, 864, 495, 5, 72, 646, 169, 438, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 158, 56, 867, 646, 1500, 393, 55, 335, 867, 140, 898, 2153, 864, 495, 5, 72, 646, 169, 438, 56, 867, 646, 1500, 393, 55, 30, 898, 2153, 864, 495, 30, 72, 646, 169, 438, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2293: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:1865: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#演示\n",
    "\n",
    "a=[]\n",
    "for key, text in train_ds[0].items():\n",
    "    print(text)\n",
    "    encoded_inputs = tokenizer(text=text, max_seq_len=512)\n",
    "    print(encoded_inputs)\n",
    "    print(encoded_inputs[\"input_ids\"])\n",
    "    print(encoded_inputs[\"token_type_ids\"])\n",
    "    a += [encoded_inputs[\"input_ids\"], encoded_inputs[\"token_type_ids\"]]\n",
    "    print('------------------------------------')\n",
    "    \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, max_seq_length=64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_fn(samples):\n",
    "    fn = Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),   \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int64'),  \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int64'),  \n",
    "    )\n",
    "\n",
    "    processed_samples = fn(samples) \n",
    "\n",
    "    result = []\n",
    "    for data in processed_samples:\n",
    "        result.append(data) \n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=64, shuffle=True)\n",
    "train_data_loader = paddle.io.DataLoader(dataset=train_ds.map(trans_func), batch_sampler=batch_sampler, collate_fn=batchify_fn, return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面开始搭建召回模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:16:54,773] [    INFO]\u001b[0m - Configuration saved in C:\\Users\\LJX\\.paddlenlp\\models\\ernie-1.0\\config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:16:55,190] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie/ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:16:55,193] [    INFO]\u001b[0m - Downloading ernie_v1_chn_base.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie/ernie_v1_chn_base.pdparams\u001b[0m\n",
      "100%|██████████| 383M/383M [08:34<00:00, 781kB/s]    \n",
      "\u001b[32m[2025-02-06 22:25:30,563] [    INFO]\u001b[0m - Loading weights file model_state.pdparams from cache at C:\\Users\\LJX\\.paddlenlp\\models\\ernie-1.0\\model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:25:30,834] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[33m[2025-02-06 22:25:31,928] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-1.0 were not used when initializing ErnieModel: ['cls.predictions.transform.bias', 'cls.predictions.decoder_bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.layer_norm.bias', 'cls.predictions.transform.weight']\n",
      "- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:25:31,929] [    INFO]\u001b[0m - All the weights of ErnieModel were initialized from the model checkpoint at ernie-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieModel for predictions without further training.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = paddlenlp.transformers.ErnieModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddle\\jit\\dy2static\\program_translator.py:712: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.\n",
      "You can set full_graph=True, then you can assign input spec.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from base_model import SemanticIndexBase\n",
    "\n",
    "class SemanticIndexBatchNeg(SemanticIndexBase): \n",
    "    def __init__(self, pretrained_model, dropout=None, margin=0.3, scale=30, output_emb_size=None):\n",
    "        super().__init__(pretrained_model, dropout, output_emb_size)\n",
    "\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, query_input_ids,    title_input_ids,    query_token_type_ids=None, query_position_ids=None, query_attention_mask=None,    title_token_type_ids=None, title_position_ids=None, title_attention_mask=None):\n",
    "        query_cls_embedding = self.get_pooled_embedding(query_input_ids, query_token_type_ids, query_position_ids, query_attention_mask) \n",
    "\n",
    "        title_cls_embedding = self.get_pooled_embedding(title_input_ids, title_token_type_ids, title_position_ids, title_attention_mask)    \n",
    "        \n",
    "        cosine_sim = paddle.matmul(query_cls_embedding, title_cls_embedding, transpose_y=True)  \n",
    "        \n",
    "        margin_diag = paddle.full(shape=[query_cls_embedding.shape[0]], fill_value=self.margin, dtype=\"float32\") \n",
    "\n",
    "        cosine_sim = cosine_sim - paddle.diag(margin_diag)\n",
    "\n",
    "        cosine_sim = cosine_sim * self.scale\n",
    "\n",
    "        labels = paddle.arange(0, query_cls_embedding.shape[0], dtype='int64') \n",
    "        labels = paddle.reshape(labels, shape=[-1, 1]) \n",
    "\n",
    "        loss = F.cross_entropy(input=cosine_sim, label=labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SemanticIndexBatchNeg(pretrained_model, margin=0.1, scale=20, output_emb_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面开始定义模型训练用到的各种参数，并进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=3 \n",
    "num_training_steps = len(train_data_loader) * epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LinearDecayWithWarmup(5E-5, num_training_steps, 0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_params = [\n",
    "        p.name for n, p in model.named_parameters() \n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"]) \n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.AdamW( \n",
    "    learning_rate=lr_scheduler, \n",
    "    parameters=model.parameters(), \n",
    "    weight_decay=0.0, \n",
    "    apply_decay_param_fun=lambda x: x in decay_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2293: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 2.26462, speed: 0.65 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:32:06,411] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:32:06,414] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 20, epoch: 1, batch: 20, loss: 1.28187, speed: 0.54 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:32:24,095] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_20\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:32:24,098] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_20\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 30, epoch: 1, batch: 30, loss: 0.99055, speed: 0.59 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:32:41,497] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_30\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:32:41,500] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_30\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 40, epoch: 1, batch: 40, loss: 1.35175, speed: 0.54 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:32:59,786] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_40\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:32:59,789] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_40\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 50, epoch: 1, batch: 50, loss: 0.76800, speed: 0.55 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:33:17,750] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_50\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:33:17,753] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_50\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 60, epoch: 1, batch: 60, loss: 0.94661, speed: 0.75 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:33:31,190] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_60\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:33:31,193] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_60\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 70, epoch: 2, batch: 7, loss: 0.70004, speed: 0.61 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:33:48,279] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_70\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:33:48,282] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_70\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 80, epoch: 2, batch: 17, loss: 0.86115, speed: 0.60 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:34:04,694] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_80\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:34:04,698] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_80\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 90, epoch: 2, batch: 27, loss: 0.87721, speed: 0.51 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:34:24,028] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_90\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:34:24,030] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_90\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100, epoch: 2, batch: 37, loss: 0.48872, speed: 0.65 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:34:39,327] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:34:39,330] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 110, epoch: 2, batch: 47, loss: 0.45475, speed: 0.62 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:34:55,671] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_110\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:34:55,677] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_110\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 120, epoch: 2, batch: 57, loss: 0.52416, speed: 0.56 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:35:13,672] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_120\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:35:13,675] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_120\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 130, epoch: 3, batch: 4, loss: 0.32604, speed: 0.51 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:35:34,102] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_130\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:35:34,104] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_130\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 140, epoch: 3, batch: 14, loss: 0.43304, speed: 0.55 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:35:51,340] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_140\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:35:51,342] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_140\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 150, epoch: 3, batch: 24, loss: 0.29230, speed: 0.61 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:36:07,104] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_150\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:36:07,106] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_150\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 160, epoch: 3, batch: 34, loss: 0.56817, speed: 0.52 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:36:27,100] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_160\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:36:27,102] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_160\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 170, epoch: 3, batch: 44, loss: 0.33112, speed: 0.71 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:36:41,463] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_170\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:36:41,465] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_170\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 180, epoch: 3, batch: 54, loss: 0.32440, speed: 0.64 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 22:36:57,125] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_180\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 22:36:57,127] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_180\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "save_dir='model_param'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "global_step = 0 \n",
    "tic_train = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1): \n",
    "    for step, batch in enumerate(train_data_loader, start=1): \n",
    "        query_input_ids, query_token_type_ids, title_input_ids, title_token_type_ids = batch \n",
    "\n",
    "        loss = model(query_input_ids=query_input_ids, title_input_ids=title_input_ids, query_token_type_ids=query_token_type_ids, title_token_type_ids=title_token_type_ids)\n",
    "\n",
    "        global_step += 1 \n",
    "        if global_step % 10 == 0: \n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, 10 / (time.time() - tic_train))) \n",
    "            tic_train = time.time() \n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        lr_scheduler.step() \n",
    "        optimizer.clear_grad() \n",
    "\n",
    "        if global_step % 10 == 0: \n",
    "            save_path = os.path.join(save_dir, \"model_%d\" % global_step)\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            save_param_path = os.path.join(save_path, 'model_state.pdparams') \n",
    "            paddle.save(model.state_dict(), save_param_path) \n",
    "            tokenizer.save_pretrained(save_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
