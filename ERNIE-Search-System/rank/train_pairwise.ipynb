{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import pandas as pd\n",
    "from data import convert_pairwise_example\n",
    "from model import PairwiseMatching\n",
    "from tqdm import tqdm\n",
    "\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import AutoModel, AutoTokenizer, LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面加载训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train(data_path): \n",
    "    with open(data_path, 'r', encoding='utf-8') as f: \n",
    "        flag = 0\n",
    "        for line in f: \n",
    "            if flag != 0: \n",
    "                data = line.rstrip().split(\"\\t\") \n",
    "                if len(data) != 3: \n",
    "                    continue\n",
    "                yield {'query': data[0], 'title': data[1], 'neg_title': data[2]} \n",
    "                                                                                   \n",
    "            flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dev(data_path): \n",
    "    with open(data_path, 'r', encoding='utf-8') as f: \n",
    "        flag = 0\n",
    "        for line in f: \n",
    "            if flag != 0: \n",
    "                data = line.rstrip().split(\"\\t\") \n",
    "                if len(data) != 3: \n",
    "                    continue\n",
    "                yield {'query': data[0], 'title': data[1], 'label': data[2]} \n",
    "                                                                               \n",
    "            flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"rank_dataset/train_pairwise.csv\"\n",
    "train_ds = load_dataset(read_train, data_path=train_file, lazy=False) \n",
    "\n",
    "dev_file = \"rank_dataset/dev_pairwise.csv\"\n",
    "dev_ds = load_dataset(read_dev, data_path=dev_file, lazy=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '英语委婉语引起的跨文化交际障碍', 'title': '英语委婉语引起的跨文化交际障碍及其翻译策略研究英语委婉语,跨文化交际障碍,翻译策略', 'neg_title': '委婉语在英语和汉语中的文化差异委婉语,文化,跨文化交际'}\n",
      "{'query': '范迪慧 嘉兴市中医院', 'title': '滋阴疏肝汤联合八穴隔姜灸治疗肾虚肝郁型卵巢功能低下的临床疗效滋阴疏肝汤,八穴隔姜灸,肾虚肝郁型卵巢功能低下,性脉甾类激素,妊娠', 'neg_title': '温针灸、中药薰蒸在半月板损伤术后康复中的疗效分析膝损伤,半月板,胫骨,中医康复,温针疗法,薰洗'}\n",
      "{'query': '灰色关联分析', 'title': '灰色关联分析评价不同产地金果榄质量金果榄;灰色关联分析法;主成分分析法;盐酸巴马汀;盐酸药根碱', 'neg_title': '江西省某三级甲等医院2型糖尿病患者次均住院费用新灰色关联分析2型糖尿病,次均住院费用,新灰色关联分析,结构变动度'}\n"
     ]
    }
   ],
   "source": [
    "# 输出训练数据集的前3条数据看一下\n",
    "for i in range(3):\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '作者单位:南州中学', 'title': '浅谈初中教学管理如何体现人文关怀初中教育,教学管理,人文关怀', 'label': '1'}\n",
      "{'query': '作者单位:南州中学', 'title': '高中美术课堂教学中藏区本土民间艺术的融入路径藏区,传统民间艺术,美术课堂', 'label': '0'}\n",
      "{'query': '作者单位:南州中学', 'title': '列宁关于资产阶级民主革命向 社会主义革命过渡的理论列宁,直接过渡,间接过渡,资产阶级民主革命,社会主义革命', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "# 输出验证数据集的前3条数据看一下\n",
    "for i in range(3):\n",
    "    print(dev_ds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 23:22:46,906] [    INFO]\u001b[0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2025-02-06 23:22:46,907] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh_vocab.txt and saved to C:\\Users\\LJX\\.paddlenlp\\models\\ernie-3.0-medium-zh\u001b[0m\n",
      "\u001b[32m[2025-02-06 23:22:47,135] [    INFO]\u001b[0m - Downloading ernie_3.0_medium_zh_vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "100%|██████████| 182k/182k [00:00<00:00, 376kB/s] \n",
      "\u001b[32m[2025-02-06 23:22:47,896] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\LJX\\.paddlenlp\\models\\ernie-3.0-medium-zh\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 23:22:47,897] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\LJX\\.paddlenlp\\models\\ernie-3.0-medium-zh\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ernie-3.0-medium-zh\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_func_train = partial(convert_pairwise_example, tokenizer=tokenizer, max_seq_length=128, phase=\"train\")\n",
    "\n",
    "trans_func_eval = partial(convert_pairwise_example, tokenizer=tokenizer, max_seq_length=128, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '英语委婉语引起的跨文化交际障碍', 'title': '英语委婉语引起的跨文化交际障碍及其翻译策略研究英语委婉语,跨文化交际障碍,翻译策略', 'neg_title': '委婉语在英语和汉语中的文化差异委婉语,文化,跨文化交际'}\n",
      "英语委婉语引起的跨文化交际障碍\n",
      "英语委婉语引起的跨文化交际障碍及其翻译策略研究英语委婉语,跨文化交际障碍,翻译策略\n",
      "委婉语在英语和汉语中的文化差异委婉语,文化,跨文化交际\n",
      "---------------------------------------------\n",
      "{'input_ids': [1, 514, 405, 298, 2681, 405, 477, 200, 5, 1465, 68, 73, 276, 430, 843, 1767, 2, 514, 405, 298, 2681, 405, 477, 200, 5, 1465, 68, 73, 276, 430, 843, 1767, 107, 63, 1197, 1285, 584, 750, 166, 229, 514, 405, 298, 2681, 405, 30, 1465, 68, 73, 276, 430, 843, 1767, 30, 1197, 1285, 584, 750, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [1, 514, 405, 298, 2681, 405, 477, 200, 5, 1465, 68, 73, 276, 430, 843, 1767, 2, 298, 2681, 405, 11, 514, 405, 14, 657, 405, 12, 5, 68, 73, 859, 712, 298, 2681, 405, 30, 68, 73, 30, 1465, 68, 73, 276, 430, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "---------------------------------------------\n",
      "[1, 514, 405, 298, 2681, 405, 477, 200, 5, 1465, 68, 73, 276, 430, 843, 1767, 2, 514, 405, 298, 2681, 405, 477, 200, 5, 1465, 68, 73, 276, 430, 843, 1767, 107, 63, 1197, 1285, 584, 750, 166, 229, 514, 405, 298, 2681, 405, 30, 1465, 68, 73, 276, 430, 843, 1767, 30, 1197, 1285, 584, 750, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "---------------------------------------------\n",
      "[1, 514, 405, 298, 2681, 405, 477, 200, 5, 1465, 68, 73, 276, 430, 843, 1767, 2, 298, 2681, 405, 11, 514, 405, 14, 657, 405, 12, 5, 68, 73, 859, 712, 298, 2681, 405, 30, 68, 73, 30, 1465, 68, 73, 276, 430, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "---------------------------------------------\n",
      "[[1, 514, 405, 298, 2681, 405, 477, 200, 5, 1465, 68, 73, 276, 430, 843, 1767, 2, 514, 405, 298, 2681, 405, 477, 200, 5, 1465, 68, 73, 276, 430, 843, 1767, 107, 63, 1197, 1285, 584, 750, 166, 229, 514, 405, 298, 2681, 405, 30, 1465, 68, 73, 276, 430, 843, 1767, 30, 1197, 1285, 584, 750, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 514, 405, 298, 2681, 405, 477, 200, 5, 1465, 68, 73, 276, 430, 843, 1767, 2, 298, 2681, 405, 11, 514, 405, 14, 657, 405, 12, 5, 68, 73, 859, 712, 298, 2681, 405, 30, 68, 73, 30, 1465, 68, 73, 276, 430, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2293: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:1865: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#演示\n",
    "example = train_ds[0]\n",
    "print(example)\n",
    "\n",
    "query, pos_title, neg_title = example[\"query\"], example[\"title\"], example[\"neg_title\"]\n",
    "print(query)\n",
    "print(pos_title)\n",
    "print(neg_title)\n",
    "print('---------------------------------------------')\n",
    "\n",
    "pos_inputs = tokenizer(text=query, text_pair=pos_title, max_seq_len=128) \n",
    "neg_inputs = tokenizer(text=query, text_pair=neg_title, max_seq_len=128) \n",
    "\n",
    "print(pos_inputs)\n",
    "print(neg_inputs)\n",
    "print('---------------------------------------------')\n",
    "\n",
    "pos_input_ids = pos_inputs[\"input_ids\"] \n",
    "pos_token_type_ids = pos_inputs[\"token_type_ids\"] \n",
    "\n",
    "print(pos_input_ids)\n",
    "print(pos_token_type_ids)\n",
    "print('---------------------------------------------')\n",
    "\n",
    "neg_input_ids = neg_inputs[\"input_ids\"] \n",
    "neg_token_type_ids = neg_inputs[\"token_type_ids\"] \n",
    "print(neg_input_ids)\n",
    "print(neg_token_type_ids)\n",
    "print('---------------------------------------------')\n",
    "\n",
    "result = [pos_input_ids, pos_token_type_ids, neg_input_ids, neg_token_type_ids]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_fn_train(samples): \n",
    "    fn = Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype=\"int64\"),  \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype=\"int64\"),  \n",
    "    )\n",
    "\n",
    "    processed_samples = fn(samples) \n",
    "\n",
    "    result = []\n",
    "    for data in processed_samples:\n",
    "        result.append(data) \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_fn_eval(samples): \n",
    "    fn = Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  \n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype=\"int64\"),  \n",
    "        Stack(dtype=\"int64\"), \n",
    "    )\n",
    "\n",
    "    processed_samples = fn(samples) \n",
    "\n",
    "    result = []\n",
    "    for data in processed_samples:\n",
    "        result.append(data) \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler_train = paddle.io.DistributedBatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "train_data_loader = paddle.io.DataLoader(dataset=train_ds.map(trans_func_train), batch_sampler=batch_sampler_train, collate_fn=batchify_fn_train, return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler_dev = paddle.io.BatchSampler(dev_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "dev_data_loader = paddle.io.DataLoader(dataset=dev_ds.map(trans_func_eval), batch_sampler=batch_sampler_dev, collate_fn=batchify_fn_eval, return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面搭建模型，并开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-06 23:28:09,072] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2025-02-06 23:28:09,075] [    INFO]\u001b[0m - Configuration saved in C:\\Users\\LJX\\.paddlenlp\\models\\ernie-3.0-medium-zh\\config.json\u001b[0m\n",
      "\u001b[32m[2025-02-06 23:28:09,301] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh.pdparams\u001b[0m\n",
      "\u001b[32m[2025-02-06 23:28:09,304] [    INFO]\u001b[0m - Downloading ernie_3.0_medium_zh.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh.pdparams\u001b[0m\n",
      "100%|██████████| 313M/313M [00:26<00:00, 12.6MB/s] \n",
      "\u001b[32m[2025-02-06 23:28:35,847] [    INFO]\u001b[0m - Loading weights file model_state.pdparams from cache at C:\\Users\\LJX\\.paddlenlp\\models\\ernie-3.0-medium-zh\\model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2025-02-06 23:28:36,067] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[33m[2025-02-06 23:28:37,078] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieModel: ['ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.linear1.bias', 'ernie.encoder.layers.6.norm1.weight']\n",
      "- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33m[2025-02-06 23:28:37,079] [ WARNING]\u001b[0m - Some weights of ErnieModel were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['ernie.pooler.dense.bias', 'ernie.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = AutoModel.from_pretrained(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PairwiseMatching(pretrained_model, margin=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3 \n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs \n",
    "\n",
    "lr_scheduler = LinearDecayWithWarmup(2E-5, num_training_steps, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters() \n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.0,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = paddle.metric.Auc() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@paddle.no_grad() \n",
    "def evaluate(model, metric, data_loader): \n",
    "    model.eval() \n",
    "    metric.reset()\n",
    " \n",
    "    for idx, batch in enumerate(data_loader): \n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        pos_probs = model.predict(input_ids=input_ids, token_type_ids=token_type_ids) \n",
    "        neg_probs = 1.0 - pos_probs \n",
    "        preds = np.concatenate((neg_probs, pos_probs), axis=1) \n",
    "\n",
    "        metric.update(preds=preds, labels=labels) \n",
    "        auc = metric.accumulate() \n",
    "\n",
    "    print(\"phase: dev, auc: {:.3}\".format(auc)) \n",
    "    metric.reset()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2293: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.09501, speed: 0.58 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.09584, speed: 0.60 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.09322, speed: 0.54 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.07434, speed: 0.68 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.09631, speed: 0.64 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.08708, speed: 0.59 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.09902, speed: 0.58 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.06268, speed: 0.59 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.07621, speed: 0.57 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2989: UserWarning: Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100, epoch: 1, batch: 100, loss: 0.08093, speed: 0.60 step/s\n",
      "phase: dev, auc: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 00:30:47,193] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 00:30:47,195] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 110, epoch: 1, batch: 110, loss: 0.08014, speed: 0.02 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.07322, speed: 0.57 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.06477, speed: 0.60 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.08893, speed: 0.51 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.06046, speed: 0.49 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.06174, speed: 0.55 step/s\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.07209, speed: 0.62 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.10872, speed: 0.59 step/s\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.07430, speed: 0.57 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.08142, speed: 0.55 step/s\n",
      "phase: dev, auc: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 00:44:27,725] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 00:44:27,728] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 210, epoch: 1, batch: 210, loss: 0.06878, speed: 0.02 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.09547, speed: 0.53 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.07929, speed: 0.49 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.06407, speed: 0.58 step/s\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.10068, speed: 0.50 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.09074, speed: 0.47 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.07885, speed: 0.47 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.08646, speed: 0.52 step/s\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.09816, speed: 0.55 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.08286, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 00:58:54,720] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 00:58:54,722] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 310, epoch: 1, batch: 310, loss: 0.07719, speed: 0.01 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.08767, speed: 0.51 step/s\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.06342, speed: 0.49 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.10204, speed: 0.48 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.05898, speed: 0.53 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.07307, speed: 0.53 step/s\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.08380, speed: 0.48 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.07718, speed: 0.44 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.06806, speed: 0.61 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.08003, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 01:11:47,603] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 01:11:47,605] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 410, epoch: 1, batch: 410, loss: 0.07639, speed: 0.02 step/s\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.08282, speed: 0.48 step/s\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.06716, speed: 0.56 step/s\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.09015, speed: 0.56 step/s\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.09065, speed: 0.58 step/s\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.06791, speed: 0.55 step/s\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.07877, speed: 0.54 step/s\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.07926, speed: 0.57 step/s\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.09907, speed: 0.67 step/s\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.08671, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 01:27:04,702] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 01:27:04,705] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 510, epoch: 1, batch: 510, loss: 0.07905, speed: 0.01 step/s\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.09019, speed: 0.55 step/s\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.08336, speed: 0.49 step/s\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.08474, speed: 0.51 step/s\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.07073, speed: 0.52 step/s\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.07792, speed: 0.50 step/s\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.08862, speed: 0.57 step/s\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.06942, speed: 0.58 step/s\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.09465, speed: 0.51 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.08300, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 01:41:23,649] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 01:41:23,651] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 610, epoch: 1, batch: 610, loss: 0.08924, speed: 0.01 step/s\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.09076, speed: 0.55 step/s\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.06908, speed: 0.55 step/s\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.06581, speed: 0.52 step/s\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.06000, speed: 0.56 step/s\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.08158, speed: 0.46 step/s\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.07324, speed: 0.46 step/s\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.08141, speed: 0.54 step/s\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.06743, speed: 0.61 step/s\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.04929, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 01:55:04,956] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 01:55:04,958] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 710, epoch: 1, batch: 710, loss: 0.09389, speed: 0.02 step/s\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.06784, speed: 0.50 step/s\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.05641, speed: 0.57 step/s\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.09464, speed: 0.48 step/s\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.08489, speed: 0.49 step/s\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.06806, speed: 0.51 step/s\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.08498, speed: 0.43 step/s\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.09056, speed: 0.58 step/s\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.09223, speed: 0.58 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.10527, speed: 0.46 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 02:10:35,118] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 02:10:35,120] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 810, epoch: 1, batch: 810, loss: 0.06414, speed: 0.01 step/s\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.06334, speed: 0.56 step/s\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.07602, speed: 0.54 step/s\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.08756, speed: 0.52 step/s\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.07873, speed: 0.57 step/s\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.06861, speed: 0.64 step/s\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.07569, speed: 0.58 step/s\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.07754, speed: 0.46 step/s\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.07945, speed: 0.52 step/s\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.09007, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 02:22:26,166] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 02:22:26,168] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 910, epoch: 1, batch: 910, loss: 0.06479, speed: 0.02 step/s\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.09393, speed: 0.56 step/s\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.08017, speed: 0.50 step/s\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.07267, speed: 0.58 step/s\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.08807, speed: 0.54 step/s\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.09111, speed: 0.57 step/s\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.08409, speed: 0.58 step/s\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.04977, speed: 0.50 step/s\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.06627, speed: 0.54 step/s\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.08677, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 02:34:21,076] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 02:34:21,078] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1010, epoch: 1, batch: 1010, loss: 0.10212, speed: 0.02 step/s\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.07681, speed: 0.51 step/s\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.06657, speed: 0.51 step/s\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.08628, speed: 0.58 step/s\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.08823, speed: 0.47 step/s\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.08734, speed: 0.52 step/s\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.06743, speed: 0.54 step/s\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.06067, speed: 0.55 step/s\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.08490, speed: 0.51 step/s\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.08334, speed: 0.55 step/s\n",
      "phase: dev, auc: 0.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 02:48:54,253] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 02:48:54,255] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1110, epoch: 1, batch: 1110, loss: 0.05680, speed: 0.01 step/s\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.06523, speed: 0.56 step/s\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.07501, speed: 0.59 step/s\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.07015, speed: 0.46 step/s\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.07740, speed: 0.48 step/s\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.07507, speed: 0.55 step/s\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.09203, speed: 0.59 step/s\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.08813, speed: 0.47 step/s\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.08627, speed: 0.56 step/s\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.07600, speed: 0.43 step/s\n",
      "phase: dev, auc: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 03:00:48,534] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 03:00:48,536] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1210, epoch: 1, batch: 1210, loss: 0.06396, speed: 0.02 step/s\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.07919, speed: 0.55 step/s\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.08445, speed: 0.57 step/s\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.07570, speed: 0.52 step/s\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.05476, speed: 0.59 step/s\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.07822, speed: 0.65 step/s\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.07094, speed: 0.62 step/s\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.06350, speed: 0.53 step/s\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.05202, speed: 0.39 step/s\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.09300, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 03:14:18,000] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 03:14:18,002] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1310, epoch: 1, batch: 1310, loss: 0.07843, speed: 0.02 step/s\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.09555, speed: 0.51 step/s\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.07747, speed: 0.57 step/s\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.07745, speed: 0.54 step/s\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.09163, speed: 0.54 step/s\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.07755, speed: 0.54 step/s\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.06448, speed: 0.58 step/s\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.08343, speed: 0.53 step/s\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.08027, speed: 0.50 step/s\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.09663, speed: 0.50 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 03:28:11,729] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 03:28:11,731] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1410, epoch: 1, batch: 1410, loss: 0.07290, speed: 0.02 step/s\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.08463, speed: 0.61 step/s\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.06114, speed: 0.52 step/s\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.08921, speed: 0.45 step/s\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.05481, speed: 0.52 step/s\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.08748, speed: 0.56 step/s\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.06246, speed: 0.52 step/s\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.06622, speed: 0.53 step/s\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.07688, speed: 0.47 step/s\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.07433, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 03:42:41,428] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 03:42:41,430] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1510, epoch: 1, batch: 1510, loss: 0.07090, speed: 0.01 step/s\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.07272, speed: 0.49 step/s\n",
      "global step 1530, epoch: 1, batch: 1530, loss: 0.07666, speed: 0.53 step/s\n",
      "global step 1540, epoch: 1, batch: 1540, loss: 0.08411, speed: 0.59 step/s\n",
      "global step 1550, epoch: 1, batch: 1550, loss: 0.06310, speed: 0.50 step/s\n",
      "global step 1560, epoch: 1, batch: 1560, loss: 0.09291, speed: 0.57 step/s\n",
      "global step 1570, epoch: 1, batch: 1570, loss: 0.07763, speed: 0.52 step/s\n",
      "global step 1580, epoch: 1, batch: 1580, loss: 0.08599, speed: 0.56 step/s\n",
      "global step 1590, epoch: 1, batch: 1590, loss: 0.05386, speed: 0.52 step/s\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.09075, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 03:54:22,904] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 03:54:22,906] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1610, epoch: 1, batch: 1610, loss: 0.07066, speed: 0.02 step/s\n",
      "global step 1620, epoch: 1, batch: 1620, loss: 0.09510, speed: 0.50 step/s\n",
      "global step 1630, epoch: 1, batch: 1630, loss: 0.08367, speed: 0.48 step/s\n",
      "global step 1640, epoch: 1, batch: 1640, loss: 0.06293, speed: 0.65 step/s\n",
      "global step 1650, epoch: 1, batch: 1650, loss: 0.08779, speed: 0.55 step/s\n",
      "global step 1660, epoch: 1, batch: 1660, loss: 0.07477, speed: 0.54 step/s\n",
      "global step 1670, epoch: 1, batch: 1670, loss: 0.07008, speed: 0.57 step/s\n",
      "global step 1680, epoch: 1, batch: 1680, loss: 0.06357, speed: 0.60 step/s\n",
      "global step 1690, epoch: 1, batch: 1690, loss: 0.06845, speed: 0.56 step/s\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.09838, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 04:05:14,953] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 04:05:14,956] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1710, epoch: 1, batch: 1710, loss: 0.09013, speed: 0.02 step/s\n",
      "global step 1720, epoch: 1, batch: 1720, loss: 0.07539, speed: 0.53 step/s\n",
      "global step 1730, epoch: 1, batch: 1730, loss: 0.06203, speed: 0.60 step/s\n",
      "global step 1740, epoch: 1, batch: 1740, loss: 0.09266, speed: 0.50 step/s\n",
      "global step 1750, epoch: 1, batch: 1750, loss: 0.06894, speed: 0.52 step/s\n",
      "global step 1760, epoch: 1, batch: 1760, loss: 0.07284, speed: 0.49 step/s\n",
      "global step 1770, epoch: 1, batch: 1770, loss: 0.07116, speed: 0.58 step/s\n",
      "global step 1780, epoch: 1, batch: 1780, loss: 0.06431, speed: 0.49 step/s\n",
      "global step 1790, epoch: 1, batch: 1790, loss: 0.07466, speed: 0.47 step/s\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.07572, speed: 0.57 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 04:18:12,272] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 04:18:12,274] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1810, epoch: 1, batch: 1810, loss: 0.07911, speed: 0.02 step/s\n",
      "global step 1820, epoch: 1, batch: 1820, loss: 0.08127, speed: 0.48 step/s\n",
      "global step 1830, epoch: 1, batch: 1830, loss: 0.06589, speed: 0.49 step/s\n",
      "global step 1840, epoch: 1, batch: 1840, loss: 0.07717, speed: 0.49 step/s\n",
      "global step 1850, epoch: 1, batch: 1850, loss: 0.07168, speed: 0.53 step/s\n",
      "global step 1860, epoch: 1, batch: 1860, loss: 0.06659, speed: 0.54 step/s\n",
      "global step 1870, epoch: 1, batch: 1870, loss: 0.10051, speed: 0.51 step/s\n",
      "global step 1880, epoch: 1, batch: 1880, loss: 0.07811, speed: 0.53 step/s\n",
      "global step 1890, epoch: 1, batch: 1890, loss: 0.07135, speed: 0.47 step/s\n",
      "global step 1900, epoch: 1, batch: 1900, loss: 0.09081, speed: 0.55 step/s\n",
      "phase: dev, auc: 0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 04:32:31,176] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_1900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 04:32:31,178] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_1900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1910, epoch: 1, batch: 1910, loss: 0.08882, speed: 0.01 step/s\n",
      "global step 1920, epoch: 1, batch: 1920, loss: 0.09404, speed: 0.53 step/s\n",
      "global step 1930, epoch: 1, batch: 1930, loss: 0.07758, speed: 0.64 step/s\n",
      "global step 1940, epoch: 1, batch: 1940, loss: 0.09294, speed: 0.52 step/s\n",
      "global step 1950, epoch: 1, batch: 1950, loss: 0.08402, speed: 0.56 step/s\n",
      "global step 1960, epoch: 1, batch: 1960, loss: 0.05842, speed: 0.53 step/s\n",
      "global step 1970, epoch: 1, batch: 1970, loss: 0.06764, speed: 0.51 step/s\n",
      "global step 1980, epoch: 1, batch: 1980, loss: 0.06097, speed: 0.58 step/s\n",
      "global step 1990, epoch: 1, batch: 1990, loss: 0.07539, speed: 0.59 step/s\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 0.04859, speed: 0.50 step/s\n",
      "phase: dev, auc: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 04:45:45,342] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 04:45:45,344] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2010, epoch: 1, batch: 2010, loss: 0.07908, speed: 0.02 step/s\n",
      "global step 2020, epoch: 1, batch: 2020, loss: 0.06411, speed: 0.53 step/s\n",
      "global step 2030, epoch: 1, batch: 2030, loss: 0.07050, speed: 0.52 step/s\n",
      "global step 2040, epoch: 1, batch: 2040, loss: 0.07740, speed: 0.54 step/s\n",
      "global step 2050, epoch: 1, batch: 2050, loss: 0.09550, speed: 0.55 step/s\n",
      "global step 2060, epoch: 1, batch: 2060, loss: 0.07563, speed: 0.50 step/s\n",
      "global step 2070, epoch: 1, batch: 2070, loss: 0.09327, speed: 0.56 step/s\n",
      "global step 2080, epoch: 1, batch: 2080, loss: 0.08984, speed: 0.57 step/s\n",
      "global step 2090, epoch: 1, batch: 2090, loss: 0.06264, speed: 0.54 step/s\n",
      "global step 2100, epoch: 1, batch: 2100, loss: 0.07978, speed: 0.66 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 04:57:44,067] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 04:57:44,069] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2110, epoch: 1, batch: 2110, loss: 0.07669, speed: 0.02 step/s\n",
      "global step 2120, epoch: 1, batch: 2120, loss: 0.06466, speed: 0.50 step/s\n",
      "global step 2130, epoch: 1, batch: 2130, loss: 0.07118, speed: 0.50 step/s\n",
      "global step 2140, epoch: 1, batch: 2140, loss: 0.06888, speed: 0.55 step/s\n",
      "global step 2150, epoch: 1, batch: 2150, loss: 0.07791, speed: 0.49 step/s\n",
      "global step 2160, epoch: 1, batch: 2160, loss: 0.08576, speed: 0.55 step/s\n",
      "global step 2170, epoch: 1, batch: 2170, loss: 0.06283, speed: 0.51 step/s\n",
      "global step 2180, epoch: 1, batch: 2180, loss: 0.08792, speed: 0.57 step/s\n",
      "global step 2190, epoch: 1, batch: 2190, loss: 0.05942, speed: 0.50 step/s\n",
      "global step 2200, epoch: 1, batch: 2200, loss: 0.06918, speed: 0.56 step/s\n",
      "phase: dev, auc: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 05:07:54,730] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 05:07:54,732] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2210, epoch: 1, batch: 2210, loss: 0.08188, speed: 0.02 step/s\n",
      "global step 2220, epoch: 1, batch: 2220, loss: 0.07206, speed: 0.48 step/s\n",
      "global step 2230, epoch: 1, batch: 2230, loss: 0.07335, speed: 0.45 step/s\n",
      "global step 2240, epoch: 1, batch: 2240, loss: 0.08900, speed: 0.56 step/s\n",
      "global step 2250, epoch: 1, batch: 2250, loss: 0.05952, speed: 0.48 step/s\n",
      "global step 2260, epoch: 1, batch: 2260, loss: 0.07917, speed: 0.46 step/s\n",
      "global step 2270, epoch: 1, batch: 2270, loss: 0.09093, speed: 0.47 step/s\n",
      "global step 2280, epoch: 1, batch: 2280, loss: 0.06788, speed: 0.54 step/s\n",
      "global step 2290, epoch: 1, batch: 2290, loss: 0.06579, speed: 0.56 step/s\n",
      "global step 2300, epoch: 1, batch: 2300, loss: 0.08264, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 05:22:05,956] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 05:22:05,959] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2310, epoch: 1, batch: 2310, loss: 0.09189, speed: 0.01 step/s\n",
      "global step 2320, epoch: 1, batch: 2320, loss: 0.07269, speed: 0.52 step/s\n",
      "global step 2330, epoch: 1, batch: 2330, loss: 0.09012, speed: 0.50 step/s\n",
      "global step 2340, epoch: 1, batch: 2340, loss: 0.07351, speed: 0.55 step/s\n",
      "global step 2350, epoch: 1, batch: 2350, loss: 0.07740, speed: 0.50 step/s\n",
      "global step 2360, epoch: 1, batch: 2360, loss: 0.07843, speed: 0.49 step/s\n",
      "global step 2370, epoch: 1, batch: 2370, loss: 0.08967, speed: 0.50 step/s\n",
      "global step 2380, epoch: 1, batch: 2380, loss: 0.06578, speed: 0.55 step/s\n",
      "global step 2390, epoch: 1, batch: 2390, loss: 0.09106, speed: 0.52 step/s\n",
      "global step 2400, epoch: 1, batch: 2400, loss: 0.06063, speed: 0.54 step/s\n",
      "phase: dev, auc: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 05:36:38,090] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 05:36:38,093] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2410, epoch: 1, batch: 2410, loss: 0.05349, speed: 0.01 step/s\n",
      "global step 2420, epoch: 1, batch: 2420, loss: 0.09169, speed: 0.57 step/s\n",
      "global step 2430, epoch: 1, batch: 2430, loss: 0.08286, speed: 0.51 step/s\n",
      "global step 2440, epoch: 1, batch: 2440, loss: 0.07001, speed: 0.62 step/s\n",
      "global step 2450, epoch: 1, batch: 2450, loss: 0.07521, speed: 0.42 step/s\n",
      "global step 2460, epoch: 1, batch: 2460, loss: 0.08026, speed: 0.53 step/s\n",
      "global step 2470, epoch: 1, batch: 2470, loss: 0.09265, speed: 0.53 step/s\n",
      "global step 2480, epoch: 1, batch: 2480, loss: 0.06603, speed: 0.61 step/s\n",
      "global step 2490, epoch: 1, batch: 2490, loss: 0.05704, speed: 0.48 step/s\n",
      "global step 2500, epoch: 1, batch: 2500, loss: 0.08065, speed: 0.55 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 05:48:27,775] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 05:48:27,777] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2510, epoch: 1, batch: 2510, loss: 0.06884, speed: 0.02 step/s\n",
      "global step 2520, epoch: 1, batch: 2520, loss: 0.08451, speed: 0.58 step/s\n",
      "global step 2530, epoch: 1, batch: 2530, loss: 0.04720, speed: 0.51 step/s\n",
      "global step 2540, epoch: 1, batch: 2540, loss: 0.06646, speed: 0.51 step/s\n",
      "global step 2550, epoch: 1, batch: 2550, loss: 0.08181, speed: 0.41 step/s\n",
      "global step 2560, epoch: 1, batch: 2560, loss: 0.06540, speed: 0.54 step/s\n",
      "global step 2570, epoch: 1, batch: 2570, loss: 0.07312, speed: 0.56 step/s\n",
      "global step 2580, epoch: 1, batch: 2580, loss: 0.06748, speed: 0.47 step/s\n",
      "global step 2590, epoch: 1, batch: 2590, loss: 0.06865, speed: 0.51 step/s\n",
      "global step 2600, epoch: 1, batch: 2600, loss: 0.07072, speed: 0.54 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 06:01:55,634] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 06:01:55,636] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2610, epoch: 1, batch: 2610, loss: 0.07659, speed: 0.02 step/s\n",
      "global step 2620, epoch: 1, batch: 2620, loss: 0.08187, speed: 0.42 step/s\n",
      "global step 2630, epoch: 1, batch: 2630, loss: 0.06640, speed: 0.49 step/s\n",
      "global step 2640, epoch: 1, batch: 2640, loss: 0.06012, speed: 0.56 step/s\n",
      "global step 2650, epoch: 1, batch: 2650, loss: 0.06928, speed: 0.47 step/s\n",
      "global step 2660, epoch: 1, batch: 2660, loss: 0.07345, speed: 0.51 step/s\n",
      "global step 2670, epoch: 1, batch: 2670, loss: 0.07809, speed: 0.57 step/s\n",
      "global step 2680, epoch: 1, batch: 2680, loss: 0.08664, speed: 0.40 step/s\n",
      "global step 2690, epoch: 1, batch: 2690, loss: 0.09603, speed: 0.47 step/s\n",
      "global step 2700, epoch: 1, batch: 2700, loss: 0.06314, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 06:16:26,247] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 06:16:26,249] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2710, epoch: 1, batch: 2710, loss: 0.07635, speed: 0.01 step/s\n",
      "global step 2720, epoch: 1, batch: 2720, loss: 0.06936, speed: 0.51 step/s\n",
      "global step 2730, epoch: 1, batch: 2730, loss: 0.05362, speed: 0.52 step/s\n",
      "global step 2740, epoch: 1, batch: 2740, loss: 0.07686, speed: 0.63 step/s\n",
      "global step 2750, epoch: 1, batch: 2750, loss: 0.09603, speed: 0.57 step/s\n",
      "global step 2760, epoch: 1, batch: 2760, loss: 0.06909, speed: 0.57 step/s\n",
      "global step 2770, epoch: 1, batch: 2770, loss: 0.07206, speed: 0.54 step/s\n",
      "global step 2780, epoch: 1, batch: 2780, loss: 0.06324, speed: 0.48 step/s\n",
      "global step 2790, epoch: 1, batch: 2790, loss: 0.07088, speed: 0.53 step/s\n",
      "global step 2800, epoch: 1, batch: 2800, loss: 0.06548, speed: 0.59 step/s\n",
      "phase: dev, auc: 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 06:29:07,695] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 06:29:07,697] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2810, epoch: 1, batch: 2810, loss: 0.07577, speed: 0.02 step/s\n",
      "global step 2820, epoch: 1, batch: 2820, loss: 0.06540, speed: 0.54 step/s\n",
      "global step 2830, epoch: 1, batch: 2830, loss: 0.06185, speed: 0.56 step/s\n",
      "global step 2840, epoch: 1, batch: 2840, loss: 0.07628, speed: 0.70 step/s\n",
      "global step 2850, epoch: 1, batch: 2850, loss: 0.09058, speed: 0.53 step/s\n",
      "global step 2860, epoch: 1, batch: 2860, loss: 0.08230, speed: 0.54 step/s\n",
      "global step 2870, epoch: 1, batch: 2870, loss: 0.09039, speed: 0.54 step/s\n",
      "global step 2880, epoch: 1, batch: 2880, loss: 0.07809, speed: 0.43 step/s\n",
      "global step 2890, epoch: 1, batch: 2890, loss: 0.08256, speed: 0.43 step/s\n",
      "global step 2900, epoch: 1, batch: 2900, loss: 0.05766, speed: 0.61 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 06:40:36,167] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_2900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 06:40:36,169] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_2900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2910, epoch: 1, batch: 2910, loss: 0.05734, speed: 0.02 step/s\n",
      "global step 2920, epoch: 1, batch: 2920, loss: 0.08302, speed: 0.53 step/s\n",
      "global step 2930, epoch: 1, batch: 2930, loss: 0.07169, speed: 0.48 step/s\n",
      "global step 2940, epoch: 1, batch: 2940, loss: 0.07209, speed: 0.56 step/s\n",
      "global step 2950, epoch: 1, batch: 2950, loss: 0.08053, speed: 0.55 step/s\n",
      "global step 2960, epoch: 1, batch: 2960, loss: 0.07196, speed: 0.53 step/s\n",
      "global step 2970, epoch: 1, batch: 2970, loss: 0.07117, speed: 0.62 step/s\n",
      "global step 2980, epoch: 1, batch: 2980, loss: 0.08669, speed: 0.45 step/s\n",
      "global step 2990, epoch: 1, batch: 2990, loss: 0.05715, speed: 0.48 step/s\n",
      "global step 3000, epoch: 1, batch: 3000, loss: 0.07619, speed: 0.48 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 06:53:23,099] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 06:53:23,101] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3010, epoch: 1, batch: 3010, loss: 0.08830, speed: 0.02 step/s\n",
      "global step 3020, epoch: 1, batch: 3020, loss: 0.07717, speed: 0.49 step/s\n",
      "global step 3030, epoch: 1, batch: 3030, loss: 0.08421, speed: 0.55 step/s\n",
      "global step 3040, epoch: 1, batch: 3040, loss: 0.06602, speed: 0.56 step/s\n",
      "global step 3050, epoch: 1, batch: 3050, loss: 0.08644, speed: 0.57 step/s\n",
      "global step 3060, epoch: 1, batch: 3060, loss: 0.08240, speed: 0.51 step/s\n",
      "global step 3070, epoch: 1, batch: 3070, loss: 0.07706, speed: 0.50 step/s\n",
      "global step 3080, epoch: 1, batch: 3080, loss: 0.09243, speed: 0.51 step/s\n",
      "global step 3090, epoch: 1, batch: 3090, loss: 0.06044, speed: 0.48 step/s\n",
      "global step 3100, epoch: 1, batch: 3100, loss: 0.07060, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 07:08:38,787] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 07:08:38,789] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3110, epoch: 1, batch: 3110, loss: 0.06871, speed: 0.01 step/s\n",
      "global step 3120, epoch: 1, batch: 3120, loss: 0.06105, speed: 0.55 step/s\n",
      "global step 3130, epoch: 1, batch: 3130, loss: 0.07205, speed: 0.54 step/s\n",
      "global step 3140, epoch: 1, batch: 3140, loss: 0.06854, speed: 0.56 step/s\n",
      "global step 3150, epoch: 1, batch: 3150, loss: 0.06224, speed: 0.49 step/s\n",
      "global step 3160, epoch: 1, batch: 3160, loss: 0.07719, speed: 0.46 step/s\n",
      "global step 3170, epoch: 1, batch: 3170, loss: 0.08227, speed: 0.50 step/s\n",
      "global step 3180, epoch: 1, batch: 3180, loss: 0.04019, speed: 0.56 step/s\n",
      "global step 3190, epoch: 1, batch: 3190, loss: 0.05030, speed: 0.57 step/s\n",
      "global step 3200, epoch: 1, batch: 3200, loss: 0.06492, speed: 0.56 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 07:21:41,462] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 07:21:41,464] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3210, epoch: 1, batch: 3210, loss: 0.08859, speed: 0.02 step/s\n",
      "global step 3220, epoch: 1, batch: 3220, loss: 0.08951, speed: 0.53 step/s\n",
      "global step 3230, epoch: 1, batch: 3230, loss: 0.07273, speed: 0.56 step/s\n",
      "global step 3240, epoch: 1, batch: 3240, loss: 0.09290, speed: 0.58 step/s\n",
      "global step 3250, epoch: 1, batch: 3250, loss: 0.05229, speed: 0.53 step/s\n",
      "global step 3260, epoch: 1, batch: 3260, loss: 0.06536, speed: 0.58 step/s\n",
      "global step 3270, epoch: 1, batch: 3270, loss: 0.07760, speed: 0.49 step/s\n",
      "global step 3280, epoch: 1, batch: 3280, loss: 0.08114, speed: 0.50 step/s\n",
      "global step 3290, epoch: 1, batch: 3290, loss: 0.07871, speed: 0.48 step/s\n",
      "global step 3300, epoch: 1, batch: 3300, loss: 0.06429, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 07:34:44,746] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 07:34:44,750] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3310, epoch: 1, batch: 3310, loss: 0.05048, speed: 0.02 step/s\n",
      "global step 3320, epoch: 1, batch: 3320, loss: 0.07091, speed: 0.52 step/s\n",
      "global step 3330, epoch: 1, batch: 3330, loss: 0.06618, speed: 0.49 step/s\n",
      "global step 3340, epoch: 1, batch: 3340, loss: 0.06974, speed: 0.51 step/s\n",
      "global step 3350, epoch: 1, batch: 3350, loss: 0.08047, speed: 0.53 step/s\n",
      "global step 3360, epoch: 1, batch: 3360, loss: 0.07622, speed: 0.44 step/s\n",
      "global step 3370, epoch: 1, batch: 3370, loss: 0.08197, speed: 0.49 step/s\n",
      "global step 3380, epoch: 1, batch: 3380, loss: 0.08018, speed: 0.49 step/s\n",
      "global step 3390, epoch: 1, batch: 3390, loss: 0.05615, speed: 0.58 step/s\n",
      "global step 3400, epoch: 1, batch: 3400, loss: 0.07114, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 07:48:21,332] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 07:48:21,334] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3410, epoch: 1, batch: 3410, loss: 0.05874, speed: 0.02 step/s\n",
      "global step 3420, epoch: 1, batch: 3420, loss: 0.05419, speed: 0.61 step/s\n",
      "global step 3430, epoch: 1, batch: 3430, loss: 0.06441, speed: 0.57 step/s\n",
      "global step 3440, epoch: 1, batch: 3440, loss: 0.06370, speed: 0.54 step/s\n",
      "global step 3450, epoch: 1, batch: 3450, loss: 0.06262, speed: 0.58 step/s\n",
      "global step 3460, epoch: 1, batch: 3460, loss: 0.08205, speed: 0.56 step/s\n",
      "global step 3470, epoch: 1, batch: 3470, loss: 0.07402, speed: 0.48 step/s\n",
      "global step 3480, epoch: 1, batch: 3480, loss: 0.09029, speed: 0.46 step/s\n",
      "global step 3490, epoch: 1, batch: 3490, loss: 0.07590, speed: 0.50 step/s\n",
      "global step 3500, epoch: 1, batch: 3500, loss: 0.08940, speed: 0.54 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 07:59:58,975] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 07:59:58,977] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3510, epoch: 1, batch: 3510, loss: 0.05503, speed: 0.02 step/s\n",
      "global step 3520, epoch: 1, batch: 3520, loss: 0.07246, speed: 0.57 step/s\n",
      "global step 3530, epoch: 1, batch: 3530, loss: 0.07008, speed: 0.51 step/s\n",
      "global step 3540, epoch: 1, batch: 3540, loss: 0.07316, speed: 0.49 step/s\n",
      "global step 3550, epoch: 1, batch: 3550, loss: 0.07128, speed: 0.52 step/s\n",
      "global step 3560, epoch: 1, batch: 3560, loss: 0.07825, speed: 0.54 step/s\n",
      "global step 3570, epoch: 1, batch: 3570, loss: 0.07104, speed: 0.61 step/s\n",
      "global step 3580, epoch: 1, batch: 3580, loss: 0.08999, speed: 0.48 step/s\n",
      "global step 3590, epoch: 1, batch: 3590, loss: 0.06924, speed: 0.56 step/s\n",
      "global step 3600, epoch: 1, batch: 3600, loss: 0.04092, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 08:12:43,385] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 08:12:43,387] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3610, epoch: 1, batch: 3610, loss: 0.09381, speed: 0.02 step/s\n",
      "global step 3620, epoch: 1, batch: 3620, loss: 0.07626, speed: 0.55 step/s\n",
      "global step 3630, epoch: 1, batch: 3630, loss: 0.07959, speed: 0.51 step/s\n",
      "global step 3640, epoch: 1, batch: 3640, loss: 0.07802, speed: 0.53 step/s\n",
      "global step 3650, epoch: 1, batch: 3650, loss: 0.08279, speed: 0.53 step/s\n",
      "global step 3660, epoch: 1, batch: 3660, loss: 0.07174, speed: 0.50 step/s\n",
      "global step 3670, epoch: 1, batch: 3670, loss: 0.06749, speed: 0.48 step/s\n",
      "global step 3680, epoch: 1, batch: 3680, loss: 0.06849, speed: 0.52 step/s\n",
      "global step 3690, epoch: 1, batch: 3690, loss: 0.09120, speed: 0.53 step/s\n",
      "global step 3700, epoch: 1, batch: 3700, loss: 0.09418, speed: 0.58 step/s\n",
      "phase: dev, auc: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 08:26:47,871] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 08:26:47,874] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3710, epoch: 1, batch: 3710, loss: 0.09038, speed: 0.01 step/s\n",
      "global step 3720, epoch: 1, batch: 3720, loss: 0.05951, speed: 0.50 step/s\n",
      "global step 3730, epoch: 1, batch: 3730, loss: 0.07800, speed: 0.45 step/s\n",
      "global step 3740, epoch: 1, batch: 3740, loss: 0.06605, speed: 0.47 step/s\n",
      "global step 3750, epoch: 1, batch: 3750, loss: 0.09906, speed: 0.50 step/s\n",
      "global step 3760, epoch: 1, batch: 3760, loss: 0.08374, speed: 0.44 step/s\n",
      "global step 3770, epoch: 1, batch: 3770, loss: 0.07128, speed: 0.57 step/s\n",
      "global step 3780, epoch: 1, batch: 3780, loss: 0.07868, speed: 0.53 step/s\n",
      "global step 3790, epoch: 1, batch: 3790, loss: 0.07234, speed: 0.52 step/s\n",
      "global step 3800, epoch: 1, batch: 3800, loss: 0.07952, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 08:40:13,617] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 08:40:13,618] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3810, epoch: 1, batch: 3810, loss: 0.08305, speed: 0.02 step/s\n",
      "global step 3820, epoch: 1, batch: 3820, loss: 0.09153, speed: 0.48 step/s\n",
      "global step 3830, epoch: 1, batch: 3830, loss: 0.06379, speed: 0.65 step/s\n",
      "global step 3840, epoch: 1, batch: 3840, loss: 0.09306, speed: 0.53 step/s\n",
      "global step 3850, epoch: 1, batch: 3850, loss: 0.06731, speed: 0.56 step/s\n",
      "global step 3860, epoch: 1, batch: 3860, loss: 0.06281, speed: 0.54 step/s\n",
      "global step 3870, epoch: 1, batch: 3870, loss: 0.07417, speed: 0.55 step/s\n",
      "global step 3880, epoch: 1, batch: 3880, loss: 0.07270, speed: 0.55 step/s\n",
      "global step 3890, epoch: 1, batch: 3890, loss: 0.07252, speed: 0.52 step/s\n",
      "global step 3900, epoch: 1, batch: 3900, loss: 0.07829, speed: 0.55 step/s\n",
      "phase: dev, auc: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 08:51:39,499] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_3900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 08:51:39,502] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_3900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 3910, epoch: 1, batch: 3910, loss: 0.08014, speed: 0.02 step/s\n",
      "global step 3920, epoch: 1, batch: 3920, loss: 0.07668, speed: 0.53 step/s\n",
      "global step 3930, epoch: 1, batch: 3930, loss: 0.09144, speed: 0.53 step/s\n",
      "global step 3940, epoch: 1, batch: 3940, loss: 0.06712, speed: 0.52 step/s\n",
      "global step 3950, epoch: 1, batch: 3950, loss: 0.06224, speed: 0.43 step/s\n",
      "global step 3960, epoch: 1, batch: 3960, loss: 0.09587, speed: 0.49 step/s\n",
      "global step 3970, epoch: 1, batch: 3970, loss: 0.06066, speed: 0.61 step/s\n",
      "global step 3980, epoch: 1, batch: 3980, loss: 0.05441, speed: 0.55 step/s\n",
      "global step 3990, epoch: 1, batch: 3990, loss: 0.07628, speed: 0.54 step/s\n",
      "global step 4000, epoch: 1, batch: 4000, loss: 0.07314, speed: 0.57 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 09:04:52,750] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 09:04:52,752] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4010, epoch: 1, batch: 4010, loss: 0.08866, speed: 0.02 step/s\n",
      "global step 4020, epoch: 1, batch: 4020, loss: 0.09478, speed: 0.54 step/s\n",
      "global step 4030, epoch: 1, batch: 4030, loss: 0.06869, speed: 0.53 step/s\n",
      "global step 4040, epoch: 1, batch: 4040, loss: 0.07372, speed: 0.53 step/s\n",
      "global step 4050, epoch: 1, batch: 4050, loss: 0.05335, speed: 0.50 step/s\n",
      "global step 4060, epoch: 1, batch: 4060, loss: 0.09710, speed: 0.50 step/s\n",
      "global step 4070, epoch: 1, batch: 4070, loss: 0.08223, speed: 0.46 step/s\n",
      "global step 4080, epoch: 1, batch: 4080, loss: 0.08360, speed: 0.53 step/s\n",
      "global step 4090, epoch: 1, batch: 4090, loss: 0.06624, speed: 0.50 step/s\n",
      "global step 4100, epoch: 1, batch: 4100, loss: 0.07691, speed: 0.56 step/s\n",
      "phase: dev, auc: 0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 09:13:57,218] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 09:13:57,220] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4110, epoch: 1, batch: 4110, loss: 0.06782, speed: 0.03 step/s\n",
      "global step 4120, epoch: 1, batch: 4120, loss: 0.07965, speed: 0.49 step/s\n",
      "global step 4130, epoch: 1, batch: 4130, loss: 0.10324, speed: 0.50 step/s\n",
      "global step 4140, epoch: 1, batch: 4140, loss: 0.07168, speed: 0.51 step/s\n",
      "global step 4150, epoch: 1, batch: 4150, loss: 0.07716, speed: 0.52 step/s\n",
      "global step 4160, epoch: 1, batch: 4160, loss: 0.07243, speed: 0.49 step/s\n",
      "global step 4170, epoch: 1, batch: 4170, loss: 0.06888, speed: 0.61 step/s\n",
      "global step 4180, epoch: 1, batch: 4180, loss: 0.07623, speed: 0.47 step/s\n",
      "global step 4190, epoch: 1, batch: 4190, loss: 0.06473, speed: 0.51 step/s\n",
      "global step 4200, epoch: 1, batch: 4200, loss: 0.07273, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 09:28:21,016] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 09:28:21,018] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4210, epoch: 1, batch: 4210, loss: 0.06503, speed: 0.01 step/s\n",
      "global step 4220, epoch: 1, batch: 4220, loss: 0.07171, speed: 0.53 step/s\n",
      "global step 4230, epoch: 1, batch: 4230, loss: 0.06503, speed: 0.59 step/s\n",
      "global step 4240, epoch: 1, batch: 4240, loss: 0.06927, speed: 0.48 step/s\n",
      "global step 4250, epoch: 1, batch: 4250, loss: 0.07308, speed: 0.59 step/s\n",
      "global step 4260, epoch: 1, batch: 4260, loss: 0.06568, speed: 0.50 step/s\n",
      "global step 4270, epoch: 1, batch: 4270, loss: 0.10054, speed: 0.55 step/s\n",
      "global step 4280, epoch: 1, batch: 4280, loss: 0.05948, speed: 0.50 step/s\n",
      "global step 4290, epoch: 1, batch: 4290, loss: 0.10415, speed: 0.54 step/s\n",
      "global step 4300, epoch: 1, batch: 4300, loss: 0.05947, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 09:42:41,456] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 09:42:41,458] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4310, epoch: 1, batch: 4310, loss: 0.08074, speed: 0.01 step/s\n",
      "global step 4320, epoch: 1, batch: 4320, loss: 0.07130, speed: 0.52 step/s\n",
      "global step 4330, epoch: 1, batch: 4330, loss: 0.08472, speed: 0.53 step/s\n",
      "global step 4340, epoch: 1, batch: 4340, loss: 0.10001, speed: 0.56 step/s\n",
      "global step 4350, epoch: 1, batch: 4350, loss: 0.07254, speed: 0.54 step/s\n",
      "global step 4360, epoch: 1, batch: 4360, loss: 0.07213, speed: 0.50 step/s\n",
      "global step 4370, epoch: 1, batch: 4370, loss: 0.07796, speed: 0.58 step/s\n",
      "global step 4380, epoch: 1, batch: 4380, loss: 0.04962, speed: 0.51 step/s\n",
      "global step 4390, epoch: 1, batch: 4390, loss: 0.05735, speed: 0.53 step/s\n",
      "global step 4400, epoch: 1, batch: 4400, loss: 0.09554, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 09:56:24,150] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 09:56:24,152] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4410, epoch: 1, batch: 4410, loss: 0.08142, speed: 0.02 step/s\n",
      "global step 4420, epoch: 1, batch: 4420, loss: 0.09636, speed: 0.51 step/s\n",
      "global step 4430, epoch: 1, batch: 4430, loss: 0.06898, speed: 0.61 step/s\n",
      "global step 4440, epoch: 1, batch: 4440, loss: 0.08011, speed: 0.53 step/s\n",
      "global step 4450, epoch: 1, batch: 4450, loss: 0.07176, speed: 0.56 step/s\n",
      "global step 4460, epoch: 1, batch: 4460, loss: 0.08307, speed: 0.48 step/s\n",
      "global step 4470, epoch: 1, batch: 4470, loss: 0.07899, speed: 0.48 step/s\n",
      "global step 4480, epoch: 1, batch: 4480, loss: 0.08565, speed: 0.50 step/s\n",
      "global step 4490, epoch: 1, batch: 4490, loss: 0.06300, speed: 0.51 step/s\n",
      "global step 4500, epoch: 1, batch: 4500, loss: 0.06923, speed: 0.47 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 10:10:41,909] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 10:10:41,911] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4510, epoch: 1, batch: 4510, loss: 0.08089, speed: 0.01 step/s\n",
      "global step 4520, epoch: 1, batch: 4520, loss: 0.07174, speed: 0.47 step/s\n",
      "global step 4530, epoch: 1, batch: 4530, loss: 0.06791, speed: 0.54 step/s\n",
      "global step 4540, epoch: 1, batch: 4540, loss: 0.07774, speed: 0.48 step/s\n",
      "global step 4550, epoch: 1, batch: 4550, loss: 0.07634, speed: 0.52 step/s\n",
      "global step 4560, epoch: 1, batch: 4560, loss: 0.05626, speed: 0.50 step/s\n",
      "global step 4570, epoch: 1, batch: 4570, loss: 0.06437, speed: 0.51 step/s\n",
      "global step 4580, epoch: 1, batch: 4580, loss: 0.09405, speed: 0.48 step/s\n",
      "global step 4590, epoch: 1, batch: 4590, loss: 0.07549, speed: 0.50 step/s\n",
      "global step 4600, epoch: 1, batch: 4600, loss: 0.07019, speed: 0.57 step/s\n",
      "phase: dev, auc: 0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 10:24:59,328] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 10:24:59,330] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4610, epoch: 1, batch: 4610, loss: 0.08607, speed: 0.01 step/s\n",
      "global step 4620, epoch: 1, batch: 4620, loss: 0.08005, speed: 0.56 step/s\n",
      "global step 4630, epoch: 1, batch: 4630, loss: 0.08139, speed: 0.55 step/s\n",
      "global step 4640, epoch: 1, batch: 4640, loss: 0.07752, speed: 0.57 step/s\n",
      "global step 4650, epoch: 1, batch: 4650, loss: 0.07790, speed: 0.50 step/s\n",
      "global step 4660, epoch: 1, batch: 4660, loss: 0.08556, speed: 0.52 step/s\n",
      "global step 4670, epoch: 1, batch: 4670, loss: 0.07978, speed: 0.54 step/s\n",
      "global step 4680, epoch: 1, batch: 4680, loss: 0.07321, speed: 0.48 step/s\n",
      "global step 4690, epoch: 1, batch: 4690, loss: 0.07998, speed: 0.59 step/s\n",
      "global step 4700, epoch: 1, batch: 4700, loss: 0.06321, speed: 0.56 step/s\n",
      "phase: dev, auc: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 10:39:22,779] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 10:39:22,781] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4710, epoch: 1, batch: 4710, loss: 0.06695, speed: 0.01 step/s\n",
      "global step 4720, epoch: 1, batch: 4720, loss: 0.07544, speed: 0.55 step/s\n",
      "global step 4730, epoch: 1, batch: 4730, loss: 0.06667, speed: 0.48 step/s\n",
      "global step 4740, epoch: 1, batch: 4740, loss: 0.06955, speed: 0.52 step/s\n",
      "global step 4750, epoch: 1, batch: 4750, loss: 0.06899, speed: 0.51 step/s\n",
      "global step 4760, epoch: 1, batch: 4760, loss: 0.09952, speed: 0.56 step/s\n",
      "global step 4770, epoch: 1, batch: 4770, loss: 0.07040, speed: 0.54 step/s\n",
      "global step 4780, epoch: 1, batch: 4780, loss: 0.06434, speed: 0.50 step/s\n",
      "global step 4790, epoch: 1, batch: 4790, loss: 0.08014, speed: 0.52 step/s\n",
      "global step 4800, epoch: 1, batch: 4800, loss: 0.06773, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 10:54:24,844] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 10:54:24,847] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4810, epoch: 1, batch: 4810, loss: 0.07939, speed: 0.01 step/s\n",
      "global step 4820, epoch: 1, batch: 4820, loss: 0.07361, speed: 0.62 step/s\n",
      "global step 4830, epoch: 1, batch: 4830, loss: 0.09031, speed: 0.53 step/s\n",
      "global step 4840, epoch: 1, batch: 4840, loss: 0.08079, speed: 0.51 step/s\n",
      "global step 4850, epoch: 1, batch: 4850, loss: 0.07445, speed: 0.64 step/s\n",
      "global step 4860, epoch: 1, batch: 4860, loss: 0.07332, speed: 0.49 step/s\n",
      "global step 4870, epoch: 1, batch: 4870, loss: 0.08329, speed: 0.56 step/s\n",
      "global step 4880, epoch: 1, batch: 4880, loss: 0.07817, speed: 0.61 step/s\n",
      "global step 4890, epoch: 1, batch: 4890, loss: 0.06551, speed: 0.58 step/s\n",
      "global step 4900, epoch: 1, batch: 4900, loss: 0.09390, speed: 0.45 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 11:07:58,613] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_4900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 11:07:58,616] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_4900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4910, epoch: 1, batch: 4910, loss: 0.08765, speed: 0.02 step/s\n",
      "global step 4920, epoch: 1, batch: 4920, loss: 0.07629, speed: 0.49 step/s\n",
      "global step 4930, epoch: 1, batch: 4930, loss: 0.07110, speed: 0.53 step/s\n",
      "global step 4940, epoch: 1, batch: 4940, loss: 0.08590, speed: 0.57 step/s\n",
      "global step 4950, epoch: 1, batch: 4950, loss: 0.06702, speed: 0.64 step/s\n",
      "global step 4960, epoch: 1, batch: 4960, loss: 0.07580, speed: 0.59 step/s\n",
      "global step 4970, epoch: 1, batch: 4970, loss: 0.07047, speed: 0.50 step/s\n",
      "global step 4980, epoch: 1, batch: 4980, loss: 0.09587, speed: 0.48 step/s\n",
      "global step 4990, epoch: 1, batch: 4990, loss: 0.06833, speed: 0.54 step/s\n",
      "global step 5000, epoch: 1, batch: 5000, loss: 0.07818, speed: 0.59 step/s\n",
      "phase: dev, auc: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 11:22:53,089] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 11:22:53,091] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5010, epoch: 1, batch: 5010, loss: 0.08144, speed: 0.01 step/s\n",
      "global step 5020, epoch: 1, batch: 5020, loss: 0.09551, speed: 0.49 step/s\n",
      "global step 5030, epoch: 1, batch: 5030, loss: 0.07397, speed: 0.65 step/s\n",
      "global step 5040, epoch: 1, batch: 5040, loss: 0.08161, speed: 0.52 step/s\n",
      "global step 5050, epoch: 1, batch: 5050, loss: 0.08878, speed: 0.55 step/s\n",
      "global step 5060, epoch: 1, batch: 5060, loss: 0.08391, speed: 0.55 step/s\n",
      "global step 5070, epoch: 1, batch: 5070, loss: 0.09257, speed: 0.66 step/s\n",
      "global step 5080, epoch: 1, batch: 5080, loss: 0.06371, speed: 0.52 step/s\n",
      "global step 5090, epoch: 1, batch: 5090, loss: 0.08466, speed: 0.52 step/s\n",
      "global step 5100, epoch: 1, batch: 5100, loss: 0.06769, speed: 0.56 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 11:36:28,707] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 11:36:28,709] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5110, epoch: 1, batch: 5110, loss: 0.08892, speed: 0.02 step/s\n",
      "global step 5120, epoch: 1, batch: 5120, loss: 0.07809, speed: 0.53 step/s\n",
      "global step 5130, epoch: 1, batch: 5130, loss: 0.08176, speed: 0.52 step/s\n",
      "global step 5140, epoch: 1, batch: 5140, loss: 0.08516, speed: 0.50 step/s\n",
      "global step 5150, epoch: 1, batch: 5150, loss: 0.05787, speed: 0.49 step/s\n",
      "global step 5160, epoch: 1, batch: 5160, loss: 0.06725, speed: 0.51 step/s\n",
      "global step 5170, epoch: 1, batch: 5170, loss: 0.06871, speed: 0.55 step/s\n",
      "global step 5180, epoch: 1, batch: 5180, loss: 0.07153, speed: 0.51 step/s\n",
      "global step 5190, epoch: 1, batch: 5190, loss: 0.03714, speed: 0.58 step/s\n",
      "global step 5200, epoch: 1, batch: 5200, loss: 0.08324, speed: 0.50 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 11:48:58,512] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 11:48:58,514] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5210, epoch: 1, batch: 5210, loss: 0.08076, speed: 0.02 step/s\n",
      "global step 5220, epoch: 1, batch: 5220, loss: 0.07584, speed: 0.57 step/s\n",
      "global step 5230, epoch: 1, batch: 5230, loss: 0.07961, speed: 0.56 step/s\n",
      "global step 5240, epoch: 1, batch: 5240, loss: 0.06321, speed: 0.54 step/s\n",
      "global step 5250, epoch: 1, batch: 5250, loss: 0.09027, speed: 0.55 step/s\n",
      "global step 5260, epoch: 1, batch: 5260, loss: 0.07706, speed: 0.53 step/s\n",
      "global step 5270, epoch: 1, batch: 5270, loss: 0.08858, speed: 0.51 step/s\n",
      "global step 5280, epoch: 1, batch: 5280, loss: 0.09630, speed: 0.53 step/s\n",
      "global step 5290, epoch: 1, batch: 5290, loss: 0.07422, speed: 0.54 step/s\n",
      "global step 5300, epoch: 1, batch: 5300, loss: 0.05788, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 12:02:43,329] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 12:02:43,332] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5310, epoch: 1, batch: 5310, loss: 0.08558, speed: 0.02 step/s\n",
      "global step 5320, epoch: 1, batch: 5320, loss: 0.07191, speed: 0.53 step/s\n",
      "global step 5330, epoch: 1, batch: 5330, loss: 0.04926, speed: 0.50 step/s\n",
      "global step 5340, epoch: 1, batch: 5340, loss: 0.07219, speed: 0.48 step/s\n",
      "global step 5350, epoch: 1, batch: 5350, loss: 0.07201, speed: 0.58 step/s\n",
      "global step 5360, epoch: 1, batch: 5360, loss: 0.06990, speed: 0.58 step/s\n",
      "global step 5370, epoch: 1, batch: 5370, loss: 0.08461, speed: 0.52 step/s\n",
      "global step 5380, epoch: 1, batch: 5380, loss: 0.07291, speed: 0.52 step/s\n",
      "global step 5390, epoch: 1, batch: 5390, loss: 0.08308, speed: 0.57 step/s\n",
      "global step 5400, epoch: 1, batch: 5400, loss: 0.05975, speed: 0.57 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 12:16:14,058] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 12:16:14,060] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5410, epoch: 1, batch: 5410, loss: 0.08018, speed: 0.02 step/s\n",
      "global step 5420, epoch: 1, batch: 5420, loss: 0.09940, speed: 0.55 step/s\n",
      "global step 5430, epoch: 1, batch: 5430, loss: 0.07879, speed: 0.56 step/s\n",
      "global step 5440, epoch: 1, batch: 5440, loss: 0.06928, speed: 0.52 step/s\n",
      "global step 5450, epoch: 1, batch: 5450, loss: 0.06369, speed: 0.51 step/s\n",
      "global step 5460, epoch: 1, batch: 5460, loss: 0.09968, speed: 0.50 step/s\n",
      "global step 5470, epoch: 1, batch: 5470, loss: 0.08570, speed: 0.54 step/s\n",
      "global step 5480, epoch: 1, batch: 5480, loss: 0.06867, speed: 0.47 step/s\n",
      "global step 5490, epoch: 1, batch: 5490, loss: 0.07750, speed: 0.52 step/s\n",
      "global step 5500, epoch: 1, batch: 5500, loss: 0.06753, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 12:28:36,010] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 12:28:36,013] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5510, epoch: 1, batch: 5510, loss: 0.06753, speed: 0.02 step/s\n",
      "global step 5520, epoch: 1, batch: 5520, loss: 0.09574, speed: 0.59 step/s\n",
      "global step 5530, epoch: 1, batch: 5530, loss: 0.07641, speed: 0.50 step/s\n",
      "global step 5540, epoch: 1, batch: 5540, loss: 0.06878, speed: 0.54 step/s\n",
      "global step 5550, epoch: 1, batch: 5550, loss: 0.07160, speed: 0.57 step/s\n",
      "global step 5560, epoch: 1, batch: 5560, loss: 0.09207, speed: 0.52 step/s\n",
      "global step 5570, epoch: 1, batch: 5570, loss: 0.06375, speed: 0.54 step/s\n",
      "global step 5580, epoch: 1, batch: 5580, loss: 0.07554, speed: 0.62 step/s\n",
      "global step 5590, epoch: 1, batch: 5590, loss: 0.07361, speed: 0.52 step/s\n",
      "global step 5600, epoch: 1, batch: 5600, loss: 0.08037, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 12:41:57,985] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 12:41:57,987] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5610, epoch: 1, batch: 5610, loss: 0.07180, speed: 0.02 step/s\n",
      "global step 5620, epoch: 1, batch: 5620, loss: 0.03613, speed: 0.54 step/s\n",
      "global step 5630, epoch: 1, batch: 5630, loss: 0.07101, speed: 0.57 step/s\n",
      "global step 5640, epoch: 1, batch: 5640, loss: 0.08759, speed: 0.52 step/s\n",
      "global step 5650, epoch: 1, batch: 5650, loss: 0.07989, speed: 0.58 step/s\n",
      "global step 5660, epoch: 1, batch: 5660, loss: 0.05731, speed: 0.61 step/s\n",
      "global step 5670, epoch: 1, batch: 5670, loss: 0.07434, speed: 0.61 step/s\n",
      "global step 5680, epoch: 1, batch: 5680, loss: 0.07812, speed: 0.55 step/s\n",
      "global step 5690, epoch: 1, batch: 5690, loss: 0.07653, speed: 0.51 step/s\n",
      "global step 5700, epoch: 1, batch: 5700, loss: 0.09473, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 12:56:40,729] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 12:56:40,731] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5710, epoch: 1, batch: 5710, loss: 0.07081, speed: 0.01 step/s\n",
      "global step 5720, epoch: 1, batch: 5720, loss: 0.08824, speed: 0.53 step/s\n",
      "global step 5730, epoch: 1, batch: 5730, loss: 0.08583, speed: 0.49 step/s\n",
      "global step 5740, epoch: 1, batch: 5740, loss: 0.08981, speed: 0.51 step/s\n",
      "global step 5750, epoch: 1, batch: 5750, loss: 0.06617, speed: 0.57 step/s\n",
      "global step 5760, epoch: 1, batch: 5760, loss: 0.08410, speed: 0.49 step/s\n",
      "global step 5770, epoch: 1, batch: 5770, loss: 0.06352, speed: 0.57 step/s\n",
      "global step 5780, epoch: 1, batch: 5780, loss: 0.09649, speed: 0.52 step/s\n",
      "global step 5790, epoch: 1, batch: 5790, loss: 0.07453, speed: 0.57 step/s\n",
      "global step 5800, epoch: 1, batch: 5800, loss: 0.08488, speed: 0.56 step/s\n",
      "phase: dev, auc: 0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 13:06:01,579] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 13:06:01,581] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5810, epoch: 1, batch: 5810, loss: 0.05522, speed: 0.03 step/s\n",
      "global step 5820, epoch: 1, batch: 5820, loss: 0.07527, speed: 0.49 step/s\n",
      "global step 5830, epoch: 1, batch: 5830, loss: 0.07109, speed: 0.50 step/s\n",
      "global step 5840, epoch: 1, batch: 5840, loss: 0.06531, speed: 0.59 step/s\n",
      "global step 5850, epoch: 1, batch: 5850, loss: 0.05476, speed: 0.53 step/s\n",
      "global step 5860, epoch: 1, batch: 5860, loss: 0.06916, speed: 0.58 step/s\n",
      "global step 5870, epoch: 1, batch: 5870, loss: 0.09892, speed: 0.54 step/s\n",
      "global step 5880, epoch: 1, batch: 5880, loss: 0.07030, speed: 0.62 step/s\n",
      "global step 5890, epoch: 1, batch: 5890, loss: 0.06937, speed: 0.51 step/s\n",
      "global step 5900, epoch: 1, batch: 5900, loss: 0.08843, speed: 0.48 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 13:20:00,146] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_5900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 13:20:00,148] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_5900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5910, epoch: 1, batch: 5910, loss: 0.06309, speed: 0.01 step/s\n",
      "global step 5920, epoch: 1, batch: 5920, loss: 0.05880, speed: 0.49 step/s\n",
      "global step 5930, epoch: 1, batch: 5930, loss: 0.08605, speed: 0.52 step/s\n",
      "global step 5940, epoch: 1, batch: 5940, loss: 0.06552, speed: 0.54 step/s\n",
      "global step 5950, epoch: 1, batch: 5950, loss: 0.07703, speed: 0.56 step/s\n",
      "global step 5960, epoch: 1, batch: 5960, loss: 0.05510, speed: 0.48 step/s\n",
      "global step 5970, epoch: 1, batch: 5970, loss: 0.06867, speed: 0.53 step/s\n",
      "global step 5980, epoch: 1, batch: 5980, loss: 0.07277, speed: 0.50 step/s\n",
      "global step 5990, epoch: 1, batch: 5990, loss: 0.06469, speed: 0.48 step/s\n",
      "global step 6000, epoch: 1, batch: 6000, loss: 0.07606, speed: 0.50 step/s\n",
      "phase: dev, auc: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 13:32:16,148] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 13:32:16,151] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6010, epoch: 1, batch: 6010, loss: 0.08244, speed: 0.02 step/s\n",
      "global step 6020, epoch: 1, batch: 6020, loss: 0.06835, speed: 0.59 step/s\n",
      "global step 6030, epoch: 1, batch: 6030, loss: 0.08326, speed: 0.47 step/s\n",
      "global step 6040, epoch: 1, batch: 6040, loss: 0.06974, speed: 0.55 step/s\n",
      "global step 6050, epoch: 1, batch: 6050, loss: 0.07855, speed: 0.56 step/s\n",
      "global step 6060, epoch: 1, batch: 6060, loss: 0.07575, speed: 0.59 step/s\n",
      "global step 6070, epoch: 1, batch: 6070, loss: 0.06288, speed: 0.55 step/s\n",
      "global step 6080, epoch: 1, batch: 6080, loss: 0.06576, speed: 0.56 step/s\n",
      "global step 6090, epoch: 1, batch: 6090, loss: 0.07134, speed: 0.56 step/s\n",
      "global step 6100, epoch: 1, batch: 6100, loss: 0.06424, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 13:46:19,507] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 13:46:19,511] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6110, epoch: 1, batch: 6110, loss: 0.05848, speed: 0.01 step/s\n",
      "global step 6120, epoch: 1, batch: 6120, loss: 0.06897, speed: 0.53 step/s\n",
      "global step 6130, epoch: 1, batch: 6130, loss: 0.06970, speed: 0.51 step/s\n",
      "global step 6140, epoch: 1, batch: 6140, loss: 0.06727, speed: 0.47 step/s\n",
      "global step 6150, epoch: 1, batch: 6150, loss: 0.06504, speed: 0.54 step/s\n",
      "global step 6160, epoch: 1, batch: 6160, loss: 0.09551, speed: 0.51 step/s\n",
      "global step 6170, epoch: 1, batch: 6170, loss: 0.07315, speed: 0.48 step/s\n",
      "global step 6180, epoch: 1, batch: 6180, loss: 0.05291, speed: 0.52 step/s\n",
      "global step 6190, epoch: 1, batch: 6190, loss: 0.08756, speed: 0.47 step/s\n",
      "global step 6200, epoch: 1, batch: 6200, loss: 0.06674, speed: 0.45 step/s\n",
      "phase: dev, auc: 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 14:01:24,660] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 14:01:24,662] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6210, epoch: 1, batch: 6210, loss: 0.06902, speed: 0.01 step/s\n",
      "global step 6220, epoch: 1, batch: 6220, loss: 0.05928, speed: 0.56 step/s\n",
      "global step 6230, epoch: 1, batch: 6230, loss: 0.08481, speed: 0.52 step/s\n",
      "global step 6240, epoch: 1, batch: 6240, loss: 0.08032, speed: 0.53 step/s\n",
      "global step 6250, epoch: 1, batch: 6250, loss: 0.07001, speed: 0.51 step/s\n",
      "global step 6260, epoch: 1, batch: 6260, loss: 0.09241, speed: 0.53 step/s\n",
      "global step 6270, epoch: 1, batch: 6270, loss: 0.08487, speed: 0.51 step/s\n",
      "global step 6280, epoch: 1, batch: 6280, loss: 0.06395, speed: 0.56 step/s\n",
      "global step 6290, epoch: 1, batch: 6290, loss: 0.09555, speed: 0.54 step/s\n",
      "global step 6300, epoch: 1, batch: 6300, loss: 0.09281, speed: 0.64 step/s\n",
      "phase: dev, auc: 0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 14:15:27,886] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 14:15:27,888] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6310, epoch: 1, batch: 6310, loss: 0.11124, speed: 0.01 step/s\n",
      "global step 6320, epoch: 1, batch: 6320, loss: 0.07246, speed: 0.54 step/s\n",
      "global step 6330, epoch: 1, batch: 6330, loss: 0.08369, speed: 0.49 step/s\n",
      "global step 6340, epoch: 1, batch: 6340, loss: 0.08235, speed: 0.47 step/s\n",
      "global step 6350, epoch: 1, batch: 6350, loss: 0.06305, speed: 0.51 step/s\n",
      "global step 6360, epoch: 1, batch: 6360, loss: 0.08674, speed: 0.49 step/s\n",
      "global step 6370, epoch: 1, batch: 6370, loss: 0.07418, speed: 0.49 step/s\n",
      "global step 6380, epoch: 1, batch: 6380, loss: 0.05930, speed: 0.54 step/s\n",
      "global step 6390, epoch: 1, batch: 6390, loss: 0.07825, speed: 0.52 step/s\n",
      "global step 6400, epoch: 1, batch: 6400, loss: 0.08507, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 14:30:34,064] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 14:30:34,065] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6410, epoch: 1, batch: 6410, loss: 0.07441, speed: 0.01 step/s\n",
      "global step 6420, epoch: 1, batch: 6420, loss: 0.07212, speed: 0.56 step/s\n",
      "global step 6430, epoch: 1, batch: 6430, loss: 0.08039, speed: 0.54 step/s\n",
      "global step 6440, epoch: 1, batch: 6440, loss: 0.06519, speed: 0.51 step/s\n",
      "global step 6450, epoch: 1, batch: 6450, loss: 0.07947, speed: 0.53 step/s\n",
      "global step 6460, epoch: 1, batch: 6460, loss: 0.09493, speed: 0.46 step/s\n",
      "global step 6470, epoch: 1, batch: 6470, loss: 0.08206, speed: 0.55 step/s\n",
      "global step 6480, epoch: 1, batch: 6480, loss: 0.06901, speed: 0.50 step/s\n",
      "global step 6490, epoch: 1, batch: 6490, loss: 0.07597, speed: 0.47 step/s\n",
      "global step 6500, epoch: 1, batch: 6500, loss: 0.09659, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 14:44:52,773] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 14:44:52,783] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6510, epoch: 1, batch: 6510, loss: 0.09430, speed: 0.01 step/s\n",
      "global step 6520, epoch: 1, batch: 6520, loss: 0.07863, speed: 0.50 step/s\n",
      "global step 6530, epoch: 1, batch: 6530, loss: 0.08580, speed: 0.54 step/s\n",
      "global step 6540, epoch: 1, batch: 6540, loss: 0.05867, speed: 0.50 step/s\n",
      "global step 6550, epoch: 1, batch: 6550, loss: 0.08002, speed: 0.54 step/s\n",
      "global step 6560, epoch: 1, batch: 6560, loss: 0.07110, speed: 0.56 step/s\n",
      "global step 6570, epoch: 1, batch: 6570, loss: 0.08177, speed: 0.49 step/s\n",
      "global step 6580, epoch: 1, batch: 6580, loss: 0.08893, speed: 0.58 step/s\n",
      "global step 6590, epoch: 1, batch: 6590, loss: 0.07047, speed: 0.54 step/s\n",
      "global step 6600, epoch: 1, batch: 6600, loss: 0.08087, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 14:57:54,088] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 14:57:54,090] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6610, epoch: 1, batch: 6610, loss: 0.07908, speed: 0.02 step/s\n",
      "global step 6620, epoch: 1, batch: 6620, loss: 0.06846, speed: 0.58 step/s\n",
      "global step 6630, epoch: 1, batch: 6630, loss: 0.05194, speed: 0.60 step/s\n",
      "global step 6640, epoch: 1, batch: 6640, loss: 0.06560, speed: 0.55 step/s\n",
      "global step 6650, epoch: 1, batch: 6650, loss: 0.07012, speed: 0.52 step/s\n",
      "global step 6660, epoch: 1, batch: 6660, loss: 0.06642, speed: 0.55 step/s\n",
      "global step 6670, epoch: 1, batch: 6670, loss: 0.07590, speed: 0.54 step/s\n",
      "global step 6680, epoch: 1, batch: 6680, loss: 0.07934, speed: 0.67 step/s\n",
      "global step 6690, epoch: 1, batch: 6690, loss: 0.06080, speed: 0.50 step/s\n",
      "global step 6700, epoch: 1, batch: 6700, loss: 0.09442, speed: 0.54 step/s\n",
      "phase: dev, auc: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 15:13:38,410] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 15:13:38,412] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6710, epoch: 1, batch: 6710, loss: 0.07736, speed: 0.01 step/s\n",
      "global step 6720, epoch: 1, batch: 6720, loss: 0.08291, speed: 0.47 step/s\n",
      "global step 6730, epoch: 1, batch: 6730, loss: 0.06688, speed: 0.62 step/s\n",
      "global step 6740, epoch: 1, batch: 6740, loss: 0.08949, speed: 0.51 step/s\n",
      "global step 6750, epoch: 1, batch: 6750, loss: 0.07316, speed: 0.49 step/s\n",
      "global step 6760, epoch: 1, batch: 6760, loss: 0.08344, speed: 0.53 step/s\n",
      "global step 6770, epoch: 1, batch: 6770, loss: 0.06931, speed: 0.54 step/s\n",
      "global step 6780, epoch: 1, batch: 6780, loss: 0.09445, speed: 0.49 step/s\n",
      "global step 6790, epoch: 1, batch: 6790, loss: 0.06840, speed: 0.49 step/s\n",
      "global step 6800, epoch: 1, batch: 6800, loss: 0.06443, speed: 0.54 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 15:27:23,337] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 15:27:23,337] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6810, epoch: 1, batch: 6810, loss: 0.08828, speed: 0.02 step/s\n",
      "global step 6820, epoch: 1, batch: 6820, loss: 0.07544, speed: 0.53 step/s\n",
      "global step 6830, epoch: 1, batch: 6830, loss: 0.08411, speed: 0.51 step/s\n",
      "global step 6840, epoch: 1, batch: 6840, loss: 0.08613, speed: 0.54 step/s\n",
      "global step 6850, epoch: 1, batch: 6850, loss: 0.06876, speed: 0.50 step/s\n",
      "global step 6860, epoch: 1, batch: 6860, loss: 0.06148, speed: 0.53 step/s\n",
      "global step 6870, epoch: 1, batch: 6870, loss: 0.06393, speed: 0.57 step/s\n",
      "global step 6880, epoch: 1, batch: 6880, loss: 0.08572, speed: 0.60 step/s\n",
      "global step 6890, epoch: 1, batch: 6890, loss: 0.06051, speed: 0.52 step/s\n",
      "global step 6900, epoch: 1, batch: 6900, loss: 0.08872, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 15:39:30,888] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_6900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 15:39:30,891] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_6900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6910, epoch: 1, batch: 6910, loss: 0.07801, speed: 0.02 step/s\n",
      "global step 6920, epoch: 1, batch: 6920, loss: 0.07795, speed: 0.53 step/s\n",
      "global step 6930, epoch: 1, batch: 6930, loss: 0.07644, speed: 0.53 step/s\n",
      "global step 6940, epoch: 1, batch: 6940, loss: 0.06961, speed: 0.57 step/s\n",
      "global step 6950, epoch: 1, batch: 6950, loss: 0.08106, speed: 0.52 step/s\n",
      "global step 6960, epoch: 1, batch: 6960, loss: 0.07147, speed: 0.58 step/s\n",
      "global step 6970, epoch: 1, batch: 6970, loss: 0.08979, speed: 0.57 step/s\n",
      "global step 6980, epoch: 1, batch: 6980, loss: 0.08345, speed: 0.49 step/s\n",
      "global step 6990, epoch: 1, batch: 6990, loss: 0.06481, speed: 0.54 step/s\n",
      "global step 7000, epoch: 1, batch: 7000, loss: 0.07686, speed: 0.50 step/s\n",
      "phase: dev, auc: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 15:52:53,611] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 15:52:53,613] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7010, epoch: 1, batch: 7010, loss: 0.04984, speed: 0.02 step/s\n",
      "global step 7020, epoch: 1, batch: 7020, loss: 0.07125, speed: 0.54 step/s\n",
      "global step 7030, epoch: 1, batch: 7030, loss: 0.07982, speed: 0.63 step/s\n",
      "global step 7040, epoch: 1, batch: 7040, loss: 0.08351, speed: 0.63 step/s\n",
      "global step 7050, epoch: 1, batch: 7050, loss: 0.07471, speed: 0.59 step/s\n",
      "global step 7060, epoch: 1, batch: 7060, loss: 0.07404, speed: 0.51 step/s\n",
      "global step 7070, epoch: 1, batch: 7070, loss: 0.07730, speed: 0.52 step/s\n",
      "global step 7080, epoch: 1, batch: 7080, loss: 0.10585, speed: 0.56 step/s\n",
      "global step 7090, epoch: 1, batch: 7090, loss: 0.08997, speed: 0.47 step/s\n",
      "global step 7100, epoch: 1, batch: 7100, loss: 0.08664, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 16:03:52,232] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 16:03:52,234] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7110, epoch: 1, batch: 7110, loss: 0.08383, speed: 0.02 step/s\n",
      "global step 7120, epoch: 1, batch: 7120, loss: 0.05593, speed: 0.52 step/s\n",
      "global step 7130, epoch: 1, batch: 7130, loss: 0.07231, speed: 0.56 step/s\n",
      "global step 7140, epoch: 1, batch: 7140, loss: 0.08043, speed: 0.61 step/s\n",
      "global step 7150, epoch: 1, batch: 7150, loss: 0.05420, speed: 0.56 step/s\n",
      "global step 7160, epoch: 1, batch: 7160, loss: 0.09018, speed: 0.54 step/s\n",
      "global step 7170, epoch: 1, batch: 7170, loss: 0.09109, speed: 0.56 step/s\n",
      "global step 7180, epoch: 1, batch: 7180, loss: 0.08177, speed: 0.50 step/s\n",
      "global step 7190, epoch: 1, batch: 7190, loss: 0.07922, speed: 0.49 step/s\n",
      "global step 7200, epoch: 1, batch: 7200, loss: 0.05915, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 16:16:00,363] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 16:16:00,364] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7210, epoch: 1, batch: 7210, loss: 0.09166, speed: 0.02 step/s\n",
      "global step 7220, epoch: 1, batch: 7220, loss: 0.06018, speed: 0.54 step/s\n",
      "global step 7230, epoch: 1, batch: 7230, loss: 0.06671, speed: 0.51 step/s\n",
      "global step 7240, epoch: 1, batch: 7240, loss: 0.08315, speed: 0.54 step/s\n",
      "global step 7250, epoch: 1, batch: 7250, loss: 0.09117, speed: 0.50 step/s\n",
      "global step 7260, epoch: 1, batch: 7260, loss: 0.08745, speed: 0.61 step/s\n",
      "global step 7270, epoch: 1, batch: 7270, loss: 0.06190, speed: 0.55 step/s\n",
      "global step 7280, epoch: 1, batch: 7280, loss: 0.04727, speed: 0.51 step/s\n",
      "global step 7290, epoch: 1, batch: 7290, loss: 0.09628, speed: 0.55 step/s\n",
      "global step 7300, epoch: 1, batch: 7300, loss: 0.07706, speed: 0.56 step/s\n",
      "phase: dev, auc: 0.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 16:28:28,785] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 16:28:28,785] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7310, epoch: 1, batch: 7310, loss: 0.06059, speed: 0.02 step/s\n",
      "global step 7320, epoch: 1, batch: 7320, loss: 0.07276, speed: 0.52 step/s\n",
      "global step 7330, epoch: 1, batch: 7330, loss: 0.07582, speed: 0.60 step/s\n",
      "global step 7340, epoch: 1, batch: 7340, loss: 0.06744, speed: 0.46 step/s\n",
      "global step 7350, epoch: 1, batch: 7350, loss: 0.07140, speed: 0.57 step/s\n",
      "global step 7360, epoch: 1, batch: 7360, loss: 0.06552, speed: 0.51 step/s\n",
      "global step 7370, epoch: 1, batch: 7370, loss: 0.07049, speed: 0.49 step/s\n",
      "global step 7380, epoch: 1, batch: 7380, loss: 0.07198, speed: 0.45 step/s\n",
      "global step 7390, epoch: 1, batch: 7390, loss: 0.09555, speed: 0.57 step/s\n",
      "global step 7400, epoch: 1, batch: 7400, loss: 0.07722, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 16:41:35,777] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 16:41:35,782] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7410, epoch: 1, batch: 7410, loss: 0.06434, speed: 0.02 step/s\n",
      "global step 7420, epoch: 1, batch: 7420, loss: 0.06529, speed: 0.59 step/s\n",
      "global step 7430, epoch: 1, batch: 7430, loss: 0.07778, speed: 0.55 step/s\n",
      "global step 7440, epoch: 1, batch: 7440, loss: 0.08112, speed: 0.57 step/s\n",
      "global step 7450, epoch: 1, batch: 7450, loss: 0.05037, speed: 0.50 step/s\n",
      "global step 7460, epoch: 1, batch: 7460, loss: 0.08823, speed: 0.48 step/s\n",
      "global step 7470, epoch: 1, batch: 7470, loss: 0.07377, speed: 0.55 step/s\n",
      "global step 7480, epoch: 1, batch: 7480, loss: 0.04897, speed: 0.55 step/s\n",
      "global step 7490, epoch: 1, batch: 7490, loss: 0.06840, speed: 0.58 step/s\n",
      "global step 7500, epoch: 1, batch: 7500, loss: 0.08318, speed: 0.47 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 16:55:22,194] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 16:55:22,197] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7510, epoch: 1, batch: 7510, loss: 0.08397, speed: 0.02 step/s\n",
      "global step 7520, epoch: 1, batch: 7520, loss: 0.07911, speed: 0.50 step/s\n",
      "global step 7530, epoch: 1, batch: 7530, loss: 0.06456, speed: 0.50 step/s\n",
      "global step 7540, epoch: 1, batch: 7540, loss: 0.09821, speed: 0.48 step/s\n",
      "global step 7550, epoch: 1, batch: 7550, loss: 0.08002, speed: 0.59 step/s\n",
      "global step 7560, epoch: 1, batch: 7560, loss: 0.06857, speed: 0.53 step/s\n",
      "global step 7570, epoch: 1, batch: 7570, loss: 0.07687, speed: 0.58 step/s\n",
      "global step 7580, epoch: 1, batch: 7580, loss: 0.06999, speed: 0.59 step/s\n",
      "global step 7590, epoch: 1, batch: 7590, loss: 0.07478, speed: 0.57 step/s\n",
      "global step 7600, epoch: 1, batch: 7600, loss: 0.05366, speed: 0.54 step/s\n",
      "phase: dev, auc: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 17:07:57,064] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 17:07:57,066] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7610, epoch: 1, batch: 7610, loss: 0.09261, speed: 0.02 step/s\n",
      "global step 7620, epoch: 1, batch: 7620, loss: 0.06106, speed: 0.57 step/s\n",
      "global step 7630, epoch: 1, batch: 7630, loss: 0.04637, speed: 0.55 step/s\n",
      "global step 7640, epoch: 1, batch: 7640, loss: 0.06080, speed: 0.55 step/s\n",
      "global step 7650, epoch: 1, batch: 7650, loss: 0.06475, speed: 0.60 step/s\n",
      "global step 7660, epoch: 1, batch: 7660, loss: 0.07939, speed: 0.63 step/s\n",
      "global step 7670, epoch: 1, batch: 7670, loss: 0.06650, speed: 0.51 step/s\n",
      "global step 7680, epoch: 1, batch: 7680, loss: 0.07358, speed: 0.58 step/s\n",
      "global step 7690, epoch: 1, batch: 7690, loss: 0.09597, speed: 0.54 step/s\n",
      "global step 7700, epoch: 1, batch: 7700, loss: 0.05441, speed: 0.55 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 17:21:26,256] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 17:21:26,258] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7710, epoch: 1, batch: 7710, loss: 0.05707, speed: 0.02 step/s\n",
      "global step 7720, epoch: 1, batch: 7720, loss: 0.06449, speed: 0.55 step/s\n",
      "global step 7730, epoch: 1, batch: 7730, loss: 0.06922, speed: 0.60 step/s\n",
      "global step 7740, epoch: 1, batch: 7740, loss: 0.07336, speed: 0.48 step/s\n",
      "global step 7750, epoch: 1, batch: 7750, loss: 0.09630, speed: 0.57 step/s\n",
      "global step 7760, epoch: 1, batch: 7760, loss: 0.05815, speed: 0.55 step/s\n",
      "global step 7770, epoch: 1, batch: 7770, loss: 0.06455, speed: 0.63 step/s\n",
      "global step 7780, epoch: 1, batch: 7780, loss: 0.05625, speed: 0.56 step/s\n",
      "global step 7790, epoch: 1, batch: 7790, loss: 0.06649, speed: 0.48 step/s\n",
      "global step 7800, epoch: 1, batch: 7800, loss: 0.08988, speed: 0.58 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 17:30:58,150] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 17:30:58,156] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7810, epoch: 1, batch: 7810, loss: 0.08364, speed: 0.02 step/s\n",
      "global step 7820, epoch: 1, batch: 7820, loss: 0.06475, speed: 0.52 step/s\n",
      "global step 7830, epoch: 1, batch: 7830, loss: 0.05903, speed: 0.46 step/s\n",
      "global step 7840, epoch: 1, batch: 7840, loss: 0.06344, speed: 0.58 step/s\n",
      "global step 7850, epoch: 1, batch: 7850, loss: 0.09372, speed: 0.49 step/s\n",
      "global step 7860, epoch: 1, batch: 7860, loss: 0.07545, speed: 0.48 step/s\n",
      "global step 7870, epoch: 1, batch: 7870, loss: 0.07660, speed: 0.49 step/s\n",
      "global step 7880, epoch: 1, batch: 7880, loss: 0.06685, speed: 0.45 step/s\n",
      "global step 7890, epoch: 1, batch: 7890, loss: 0.07651, speed: 0.60 step/s\n",
      "global step 7900, epoch: 1, batch: 7900, loss: 0.07178, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 17:39:04,286] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_7900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 17:39:04,289] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_7900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7910, epoch: 1, batch: 7910, loss: 0.05005, speed: 0.03 step/s\n",
      "global step 7920, epoch: 1, batch: 7920, loss: 0.05041, speed: 0.57 step/s\n",
      "global step 7930, epoch: 1, batch: 7930, loss: 0.07076, speed: 0.51 step/s\n",
      "global step 7940, epoch: 1, batch: 7940, loss: 0.08733, speed: 0.50 step/s\n",
      "global step 7950, epoch: 1, batch: 7950, loss: 0.08040, speed: 0.53 step/s\n",
      "global step 7960, epoch: 1, batch: 7960, loss: 0.06577, speed: 0.54 step/s\n",
      "global step 7970, epoch: 1, batch: 7970, loss: 0.05978, speed: 0.59 step/s\n",
      "global step 7980, epoch: 1, batch: 7980, loss: 0.10517, speed: 0.53 step/s\n",
      "global step 7990, epoch: 1, batch: 7990, loss: 0.06540, speed: 0.57 step/s\n",
      "global step 8000, epoch: 1, batch: 8000, loss: 0.06499, speed: 0.50 step/s\n",
      "phase: dev, auc: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 17:57:07,092] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 17:57:07,092] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8010, epoch: 1, batch: 8010, loss: 0.06372, speed: 0.01 step/s\n",
      "global step 8020, epoch: 1, batch: 8020, loss: 0.07718, speed: 0.51 step/s\n",
      "global step 8030, epoch: 1, batch: 8030, loss: 0.07227, speed: 0.52 step/s\n",
      "global step 8040, epoch: 1, batch: 8040, loss: 0.07793, speed: 0.62 step/s\n",
      "global step 8050, epoch: 1, batch: 8050, loss: 0.05041, speed: 0.57 step/s\n",
      "global step 8060, epoch: 1, batch: 8060, loss: 0.09744, speed: 0.51 step/s\n",
      "global step 8070, epoch: 1, batch: 8070, loss: 0.06132, speed: 0.51 step/s\n",
      "global step 8080, epoch: 1, batch: 8080, loss: 0.08445, speed: 0.58 step/s\n",
      "global step 8090, epoch: 1, batch: 8090, loss: 0.09030, speed: 0.46 step/s\n",
      "global step 8100, epoch: 1, batch: 8100, loss: 0.08403, speed: 0.59 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 18:10:22,305] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 18:10:22,308] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8110, epoch: 1, batch: 8110, loss: 0.06639, speed: 0.02 step/s\n",
      "global step 8120, epoch: 1, batch: 8120, loss: 0.07036, speed: 0.60 step/s\n",
      "global step 8130, epoch: 1, batch: 8130, loss: 0.07940, speed: 0.52 step/s\n",
      "global step 8140, epoch: 1, batch: 8140, loss: 0.07489, speed: 0.52 step/s\n",
      "global step 8150, epoch: 1, batch: 8150, loss: 0.07036, speed: 0.55 step/s\n",
      "global step 8160, epoch: 1, batch: 8160, loss: 0.06616, speed: 0.58 step/s\n",
      "global step 8170, epoch: 1, batch: 8170, loss: 0.06531, speed: 0.53 step/s\n",
      "global step 8180, epoch: 1, batch: 8180, loss: 0.06740, speed: 0.61 step/s\n",
      "global step 8190, epoch: 1, batch: 8190, loss: 0.06593, speed: 0.54 step/s\n",
      "global step 8200, epoch: 1, batch: 8200, loss: 0.08086, speed: 0.56 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 18:25:46,742] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 18:25:46,744] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8210, epoch: 1, batch: 8210, loss: 0.06577, speed: 0.01 step/s\n",
      "global step 8220, epoch: 1, batch: 8220, loss: 0.07527, speed: 0.55 step/s\n",
      "global step 8230, epoch: 1, batch: 8230, loss: 0.08342, speed: 0.51 step/s\n",
      "global step 8240, epoch: 1, batch: 8240, loss: 0.06129, speed: 0.56 step/s\n",
      "global step 8250, epoch: 1, batch: 8250, loss: 0.07028, speed: 0.53 step/s\n",
      "global step 8260, epoch: 1, batch: 8260, loss: 0.06833, speed: 0.56 step/s\n",
      "global step 8270, epoch: 1, batch: 8270, loss: 0.09330, speed: 0.50 step/s\n",
      "global step 8280, epoch: 1, batch: 8280, loss: 0.08155, speed: 0.53 step/s\n",
      "global step 8290, epoch: 1, batch: 8290, loss: 0.06195, speed: 0.54 step/s\n",
      "global step 8300, epoch: 1, batch: 8300, loss: 0.07489, speed: 0.48 step/s\n",
      "phase: dev, auc: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 18:40:36,834] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 18:40:36,837] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8310, epoch: 1, batch: 8310, loss: 0.08716, speed: 0.01 step/s\n",
      "global step 8320, epoch: 1, batch: 8320, loss: 0.06607, speed: 0.54 step/s\n",
      "global step 8330, epoch: 1, batch: 8330, loss: 0.06746, speed: 0.56 step/s\n",
      "global step 8340, epoch: 1, batch: 8340, loss: 0.07492, speed: 0.63 step/s\n",
      "global step 8350, epoch: 1, batch: 8350, loss: 0.07205, speed: 0.49 step/s\n",
      "global step 8360, epoch: 1, batch: 8360, loss: 0.06015, speed: 0.53 step/s\n",
      "global step 8370, epoch: 1, batch: 8370, loss: 0.09575, speed: 0.46 step/s\n",
      "global step 8380, epoch: 1, batch: 8380, loss: 0.06060, speed: 0.63 step/s\n",
      "global step 8390, epoch: 1, batch: 8390, loss: 0.06900, speed: 0.68 step/s\n",
      "global step 8400, epoch: 1, batch: 8400, loss: 0.09750, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 18:53:44,511] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 18:53:44,514] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8410, epoch: 1, batch: 8410, loss: 0.09184, speed: 0.02 step/s\n",
      "global step 8420, epoch: 1, batch: 8420, loss: 0.07328, speed: 0.55 step/s\n",
      "global step 8430, epoch: 1, batch: 8430, loss: 0.08009, speed: 0.52 step/s\n",
      "global step 8440, epoch: 1, batch: 8440, loss: 0.05470, speed: 0.60 step/s\n",
      "global step 8450, epoch: 1, batch: 8450, loss: 0.06232, speed: 0.49 step/s\n",
      "global step 8460, epoch: 1, batch: 8460, loss: 0.07919, speed: 0.49 step/s\n",
      "global step 8470, epoch: 1, batch: 8470, loss: 0.09254, speed: 0.55 step/s\n",
      "global step 8480, epoch: 1, batch: 8480, loss: 0.09153, speed: 0.50 step/s\n",
      "global step 8490, epoch: 1, batch: 8490, loss: 0.06884, speed: 0.47 step/s\n",
      "global step 8500, epoch: 1, batch: 8500, loss: 0.06804, speed: 0.58 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 19:04:58,514] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 19:04:58,516] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8510, epoch: 1, batch: 8510, loss: 0.07620, speed: 0.02 step/s\n",
      "global step 8520, epoch: 1, batch: 8520, loss: 0.07545, speed: 0.52 step/s\n",
      "global step 8530, epoch: 1, batch: 8530, loss: 0.05471, speed: 0.54 step/s\n",
      "global step 8540, epoch: 1, batch: 8540, loss: 0.05060, speed: 0.56 step/s\n",
      "global step 8550, epoch: 1, batch: 8550, loss: 0.08730, speed: 0.61 step/s\n",
      "global step 8560, epoch: 1, batch: 8560, loss: 0.07722, speed: 0.53 step/s\n",
      "global step 8570, epoch: 1, batch: 8570, loss: 0.10190, speed: 0.62 step/s\n",
      "global step 8580, epoch: 1, batch: 8580, loss: 0.05534, speed: 0.53 step/s\n",
      "global step 8590, epoch: 1, batch: 8590, loss: 0.07849, speed: 0.51 step/s\n",
      "global step 8600, epoch: 1, batch: 8600, loss: 0.07550, speed: 0.57 step/s\n",
      "phase: dev, auc: 0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 19:21:48,764] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 19:21:48,766] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8610, epoch: 1, batch: 8610, loss: 0.08417, speed: 0.01 step/s\n",
      "global step 8620, epoch: 1, batch: 8620, loss: 0.08357, speed: 0.56 step/s\n",
      "global step 8630, epoch: 1, batch: 8630, loss: 0.08550, speed: 0.57 step/s\n",
      "global step 8640, epoch: 1, batch: 8640, loss: 0.08011, speed: 0.55 step/s\n",
      "global step 8650, epoch: 1, batch: 8650, loss: 0.09811, speed: 0.54 step/s\n",
      "global step 8660, epoch: 1, batch: 8660, loss: 0.05093, speed: 0.60 step/s\n",
      "global step 8670, epoch: 1, batch: 8670, loss: 0.07558, speed: 0.56 step/s\n",
      "global step 8680, epoch: 1, batch: 8680, loss: 0.09395, speed: 0.64 step/s\n",
      "global step 8690, epoch: 1, batch: 8690, loss: 0.07740, speed: 0.47 step/s\n",
      "global step 8700, epoch: 1, batch: 8700, loss: 0.09188, speed: 0.49 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 19:33:32,435] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 19:33:32,437] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8710, epoch: 1, batch: 8710, loss: 0.06259, speed: 0.02 step/s\n",
      "global step 8720, epoch: 1, batch: 8720, loss: 0.08800, speed: 0.56 step/s\n",
      "global step 8730, epoch: 1, batch: 8730, loss: 0.10633, speed: 0.52 step/s\n",
      "global step 8740, epoch: 1, batch: 8740, loss: 0.08985, speed: 0.44 step/s\n",
      "global step 8750, epoch: 1, batch: 8750, loss: 0.06391, speed: 0.54 step/s\n",
      "global step 8760, epoch: 1, batch: 8760, loss: 0.08110, speed: 0.47 step/s\n",
      "global step 8770, epoch: 1, batch: 8770, loss: 0.08411, speed: 0.50 step/s\n",
      "global step 8780, epoch: 1, batch: 8780, loss: 0.05026, speed: 0.51 step/s\n",
      "global step 8790, epoch: 1, batch: 8790, loss: 0.08227, speed: 0.53 step/s\n",
      "global step 8800, epoch: 1, batch: 8800, loss: 0.06846, speed: 0.55 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 19:48:32,775] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 19:48:32,777] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8810, epoch: 1, batch: 8810, loss: 0.07930, speed: 0.01 step/s\n",
      "global step 8820, epoch: 1, batch: 8820, loss: 0.07797, speed: 0.56 step/s\n",
      "global step 8830, epoch: 1, batch: 8830, loss: 0.08213, speed: 0.51 step/s\n",
      "global step 8840, epoch: 1, batch: 8840, loss: 0.07389, speed: 0.41 step/s\n",
      "global step 8850, epoch: 1, batch: 8850, loss: 0.07109, speed: 0.58 step/s\n",
      "global step 8860, epoch: 1, batch: 8860, loss: 0.06969, speed: 0.52 step/s\n",
      "global step 8870, epoch: 1, batch: 8870, loss: 0.06741, speed: 0.51 step/s\n",
      "global step 8880, epoch: 1, batch: 8880, loss: 0.07685, speed: 0.51 step/s\n",
      "global step 8890, epoch: 1, batch: 8890, loss: 0.07370, speed: 0.57 step/s\n",
      "global step 8900, epoch: 1, batch: 8900, loss: 0.08443, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 20:00:43,871] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_8900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 20:00:43,873] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_8900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8910, epoch: 1, batch: 8910, loss: 0.07686, speed: 0.02 step/s\n",
      "global step 8920, epoch: 1, batch: 8920, loss: 0.07212, speed: 0.57 step/s\n",
      "global step 8930, epoch: 1, batch: 8930, loss: 0.06339, speed: 0.50 step/s\n",
      "global step 8940, epoch: 1, batch: 8940, loss: 0.07724, speed: 0.55 step/s\n",
      "global step 8950, epoch: 1, batch: 8950, loss: 0.09105, speed: 0.58 step/s\n",
      "global step 8960, epoch: 1, batch: 8960, loss: 0.08361, speed: 0.52 step/s\n",
      "global step 8970, epoch: 1, batch: 8970, loss: 0.08568, speed: 0.60 step/s\n",
      "global step 8980, epoch: 1, batch: 8980, loss: 0.07137, speed: 0.55 step/s\n",
      "global step 8990, epoch: 1, batch: 8990, loss: 0.08892, speed: 0.44 step/s\n",
      "global step 9000, epoch: 1, batch: 9000, loss: 0.08191, speed: 0.58 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 20:15:16,570] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 20:15:16,572] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9010, epoch: 1, batch: 9010, loss: 0.07286, speed: 0.01 step/s\n",
      "global step 9020, epoch: 1, batch: 9020, loss: 0.08686, speed: 0.54 step/s\n",
      "global step 9030, epoch: 1, batch: 9030, loss: 0.06264, speed: 0.55 step/s\n",
      "global step 9040, epoch: 1, batch: 9040, loss: 0.08737, speed: 0.50 step/s\n",
      "global step 9050, epoch: 1, batch: 9050, loss: 0.07188, speed: 0.50 step/s\n",
      "global step 9060, epoch: 1, batch: 9060, loss: 0.06988, speed: 0.58 step/s\n",
      "global step 9070, epoch: 1, batch: 9070, loss: 0.08168, speed: 0.50 step/s\n",
      "global step 9080, epoch: 1, batch: 9080, loss: 0.07565, speed: 0.50 step/s\n",
      "global step 9090, epoch: 1, batch: 9090, loss: 0.08350, speed: 0.54 step/s\n",
      "global step 9100, epoch: 1, batch: 9100, loss: 0.05307, speed: 0.55 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 20:29:42,292] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 20:29:42,294] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9110, epoch: 1, batch: 9110, loss: 0.07832, speed: 0.01 step/s\n",
      "global step 9120, epoch: 1, batch: 9120, loss: 0.08352, speed: 0.48 step/s\n",
      "global step 9130, epoch: 1, batch: 9130, loss: 0.09220, speed: 0.58 step/s\n",
      "global step 9140, epoch: 1, batch: 9140, loss: 0.06884, speed: 0.46 step/s\n",
      "global step 9150, epoch: 1, batch: 9150, loss: 0.08910, speed: 0.50 step/s\n",
      "global step 9160, epoch: 1, batch: 9160, loss: 0.06101, speed: 0.57 step/s\n",
      "global step 9170, epoch: 1, batch: 9170, loss: 0.08848, speed: 0.53 step/s\n",
      "global step 9180, epoch: 1, batch: 9180, loss: 0.05218, speed: 0.50 step/s\n",
      "global step 9190, epoch: 1, batch: 9190, loss: 0.07554, speed: 0.54 step/s\n",
      "global step 9200, epoch: 1, batch: 9200, loss: 0.06030, speed: 0.56 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 20:44:59,337] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 20:44:59,339] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9210, epoch: 1, batch: 9210, loss: 0.08841, speed: 0.01 step/s\n",
      "global step 9220, epoch: 1, batch: 9220, loss: 0.04799, speed: 0.50 step/s\n",
      "global step 9230, epoch: 1, batch: 9230, loss: 0.09453, speed: 0.56 step/s\n",
      "global step 9240, epoch: 1, batch: 9240, loss: 0.08171, speed: 0.61 step/s\n",
      "global step 9250, epoch: 1, batch: 9250, loss: 0.07277, speed: 0.56 step/s\n",
      "global step 9260, epoch: 1, batch: 9260, loss: 0.07430, speed: 0.56 step/s\n",
      "global step 9270, epoch: 1, batch: 9270, loss: 0.08566, speed: 0.55 step/s\n",
      "global step 9280, epoch: 1, batch: 9280, loss: 0.08409, speed: 0.58 step/s\n",
      "global step 9290, epoch: 1, batch: 9290, loss: 0.07561, speed: 0.49 step/s\n",
      "global step 9300, epoch: 1, batch: 9300, loss: 0.07362, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 20:57:19,480] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 20:57:19,482] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9310, epoch: 1, batch: 9310, loss: 0.09434, speed: 0.02 step/s\n",
      "global step 9320, epoch: 1, batch: 9320, loss: 0.08437, speed: 0.56 step/s\n",
      "global step 9330, epoch: 1, batch: 9330, loss: 0.08529, speed: 0.57 step/s\n",
      "global step 9340, epoch: 1, batch: 9340, loss: 0.08235, speed: 0.50 step/s\n",
      "global step 9350, epoch: 1, batch: 9350, loss: 0.07147, speed: 0.58 step/s\n",
      "global step 9360, epoch: 1, batch: 9360, loss: 0.08904, speed: 0.51 step/s\n",
      "global step 9370, epoch: 1, batch: 9370, loss: 0.08736, speed: 0.53 step/s\n",
      "global step 9380, epoch: 1, batch: 9380, loss: 0.07031, speed: 0.47 step/s\n",
      "global step 9390, epoch: 1, batch: 9390, loss: 0.08064, speed: 0.58 step/s\n",
      "global step 9400, epoch: 1, batch: 9400, loss: 0.08534, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 21:09:08,460] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 21:09:08,462] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9410, epoch: 1, batch: 9410, loss: 0.08557, speed: 0.02 step/s\n",
      "global step 9420, epoch: 1, batch: 9420, loss: 0.07038, speed: 0.48 step/s\n",
      "global step 9430, epoch: 1, batch: 9430, loss: 0.08211, speed: 0.57 step/s\n",
      "global step 9440, epoch: 1, batch: 9440, loss: 0.06361, speed: 0.51 step/s\n",
      "global step 9450, epoch: 1, batch: 9450, loss: 0.06736, speed: 0.46 step/s\n",
      "global step 9460, epoch: 1, batch: 9460, loss: 0.08116, speed: 0.53 step/s\n",
      "global step 9470, epoch: 1, batch: 9470, loss: 0.06584, speed: 0.56 step/s\n",
      "global step 9480, epoch: 1, batch: 9480, loss: 0.06170, speed: 0.50 step/s\n",
      "global step 9490, epoch: 1, batch: 9490, loss: 0.07175, speed: 0.50 step/s\n",
      "global step 9500, epoch: 1, batch: 9500, loss: 0.07106, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 21:18:42,399] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 21:18:42,401] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9510, epoch: 1, batch: 9510, loss: 0.08501, speed: 0.03 step/s\n",
      "global step 9520, epoch: 1, batch: 9520, loss: 0.04735, speed: 0.57 step/s\n",
      "global step 9530, epoch: 1, batch: 9530, loss: 0.07708, speed: 0.52 step/s\n",
      "global step 9540, epoch: 1, batch: 9540, loss: 0.08714, speed: 0.52 step/s\n",
      "global step 9550, epoch: 1, batch: 9550, loss: 0.07654, speed: 0.50 step/s\n",
      "global step 9560, epoch: 1, batch: 9560, loss: 0.08582, speed: 0.44 step/s\n",
      "global step 9570, epoch: 1, batch: 9570, loss: 0.08491, speed: 0.49 step/s\n",
      "global step 9580, epoch: 1, batch: 9580, loss: 0.10098, speed: 0.52 step/s\n",
      "global step 9590, epoch: 1, batch: 9590, loss: 0.07216, speed: 0.53 step/s\n",
      "global step 9600, epoch: 1, batch: 9600, loss: 0.06225, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 21:32:30,444] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 21:32:30,447] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9610, epoch: 1, batch: 9610, loss: 0.07565, speed: 0.02 step/s\n",
      "global step 9620, epoch: 1, batch: 9620, loss: 0.08809, speed: 0.55 step/s\n",
      "global step 9630, epoch: 1, batch: 9630, loss: 0.08765, speed: 0.56 step/s\n",
      "global step 9640, epoch: 1, batch: 9640, loss: 0.07077, speed: 0.56 step/s\n",
      "global step 9650, epoch: 1, batch: 9650, loss: 0.07365, speed: 0.57 step/s\n",
      "global step 9660, epoch: 1, batch: 9660, loss: 0.06485, speed: 0.59 step/s\n",
      "global step 9670, epoch: 1, batch: 9670, loss: 0.08167, speed: 0.53 step/s\n",
      "global step 9680, epoch: 1, batch: 9680, loss: 0.06979, speed: 0.55 step/s\n",
      "global step 9690, epoch: 1, batch: 9690, loss: 0.08253, speed: 0.53 step/s\n",
      "global step 9700, epoch: 1, batch: 9700, loss: 0.08736, speed: 0.48 step/s\n",
      "phase: dev, auc: 0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 21:45:26,059] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 21:45:26,061] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9710, epoch: 1, batch: 9710, loss: 0.08018, speed: 0.02 step/s\n",
      "global step 9720, epoch: 1, batch: 9720, loss: 0.07266, speed: 0.58 step/s\n",
      "global step 9730, epoch: 1, batch: 9730, loss: 0.08180, speed: 0.57 step/s\n",
      "global step 9740, epoch: 1, batch: 9740, loss: 0.06481, speed: 0.54 step/s\n",
      "global step 9750, epoch: 1, batch: 9750, loss: 0.07921, speed: 0.55 step/s\n",
      "global step 9760, epoch: 1, batch: 9760, loss: 0.07620, speed: 0.49 step/s\n",
      "global step 9770, epoch: 1, batch: 9770, loss: 0.06998, speed: 0.62 step/s\n",
      "global step 9780, epoch: 1, batch: 9780, loss: 0.10274, speed: 0.53 step/s\n",
      "global step 9790, epoch: 1, batch: 9790, loss: 0.09465, speed: 0.56 step/s\n",
      "global step 9800, epoch: 1, batch: 9800, loss: 0.06692, speed: 0.54 step/s\n",
      "phase: dev, auc: 0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 21:59:00,885] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 21:59:00,885] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9810, epoch: 1, batch: 9810, loss: 0.08953, speed: 0.02 step/s\n",
      "global step 9820, epoch: 1, batch: 9820, loss: 0.07544, speed: 0.59 step/s\n",
      "global step 9830, epoch: 1, batch: 9830, loss: 0.09855, speed: 0.53 step/s\n",
      "global step 9840, epoch: 1, batch: 9840, loss: 0.07702, speed: 0.60 step/s\n",
      "global step 9850, epoch: 1, batch: 9850, loss: 0.07749, speed: 0.51 step/s\n",
      "global step 9860, epoch: 1, batch: 9860, loss: 0.08791, speed: 0.51 step/s\n",
      "global step 9870, epoch: 1, batch: 9870, loss: 0.07208, speed: 0.59 step/s\n",
      "global step 9880, epoch: 1, batch: 9880, loss: 0.06694, speed: 0.58 step/s\n",
      "global step 9890, epoch: 1, batch: 9890, loss: 0.08764, speed: 0.61 step/s\n",
      "global step 9900, epoch: 1, batch: 9900, loss: 0.06772, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 22:10:53,920] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_9900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 22:10:53,922] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_9900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 9910, epoch: 1, batch: 9910, loss: 0.06333, speed: 0.02 step/s\n",
      "global step 9920, epoch: 1, batch: 9920, loss: 0.07181, speed: 0.51 step/s\n",
      "global step 9930, epoch: 1, batch: 9930, loss: 0.07383, speed: 0.55 step/s\n",
      "global step 9940, epoch: 1, batch: 9940, loss: 0.07092, speed: 0.56 step/s\n",
      "global step 9950, epoch: 1, batch: 9950, loss: 0.07980, speed: 0.55 step/s\n",
      "global step 9960, epoch: 1, batch: 9960, loss: 0.06961, speed: 0.58 step/s\n",
      "global step 9970, epoch: 1, batch: 9970, loss: 0.05914, speed: 0.54 step/s\n",
      "global step 9980, epoch: 1, batch: 9980, loss: 0.05792, speed: 0.56 step/s\n",
      "global step 9990, epoch: 1, batch: 9990, loss: 0.10015, speed: 0.53 step/s\n",
      "global step 10000, epoch: 1, batch: 10000, loss: 0.07370, speed: 0.48 step/s\n",
      "phase: dev, auc: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 22:24:04,411] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10000\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 22:24:04,411] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10000\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10010, epoch: 1, batch: 10010, loss: 0.09527, speed: 0.02 step/s\n",
      "global step 10020, epoch: 1, batch: 10020, loss: 0.06559, speed: 0.56 step/s\n",
      "global step 10030, epoch: 1, batch: 10030, loss: 0.06614, speed: 0.57 step/s\n",
      "global step 10040, epoch: 1, batch: 10040, loss: 0.06515, speed: 0.51 step/s\n",
      "global step 10050, epoch: 1, batch: 10050, loss: 0.07304, speed: 0.61 step/s\n",
      "global step 10060, epoch: 1, batch: 10060, loss: 0.06717, speed: 0.51 step/s\n",
      "global step 10070, epoch: 1, batch: 10070, loss: 0.06463, speed: 0.50 step/s\n",
      "global step 10080, epoch: 1, batch: 10080, loss: 0.07471, speed: 0.53 step/s\n",
      "global step 10090, epoch: 1, batch: 10090, loss: 0.09089, speed: 0.56 step/s\n",
      "global step 10100, epoch: 1, batch: 10100, loss: 0.06542, speed: 0.69 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 22:36:23,665] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10100\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 22:36:23,665] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10100\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10110, epoch: 1, batch: 10110, loss: 0.07865, speed: 0.02 step/s\n",
      "global step 10120, epoch: 1, batch: 10120, loss: 0.07635, speed: 0.59 step/s\n",
      "global step 10130, epoch: 1, batch: 10130, loss: 0.05080, speed: 0.64 step/s\n",
      "global step 10140, epoch: 1, batch: 10140, loss: 0.06767, speed: 0.54 step/s\n",
      "global step 10150, epoch: 1, batch: 10150, loss: 0.06825, speed: 0.52 step/s\n",
      "global step 10160, epoch: 1, batch: 10160, loss: 0.07007, speed: 0.52 step/s\n",
      "global step 10170, epoch: 1, batch: 10170, loss: 0.08580, speed: 0.59 step/s\n",
      "global step 10180, epoch: 1, batch: 10180, loss: 0.07995, speed: 0.57 step/s\n",
      "global step 10190, epoch: 1, batch: 10190, loss: 0.09146, speed: 0.66 step/s\n",
      "global step 10200, epoch: 1, batch: 10200, loss: 0.08387, speed: 0.62 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 22:47:35,316] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10200\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 22:47:35,320] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10200\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10210, epoch: 1, batch: 10210, loss: 0.07780, speed: 0.02 step/s\n",
      "global step 10220, epoch: 1, batch: 10220, loss: 0.08956, speed: 0.54 step/s\n",
      "global step 10230, epoch: 1, batch: 10230, loss: 0.08969, speed: 0.58 step/s\n",
      "global step 10240, epoch: 1, batch: 10240, loss: 0.03845, speed: 0.56 step/s\n",
      "global step 10250, epoch: 1, batch: 10250, loss: 0.07863, speed: 0.48 step/s\n",
      "global step 10260, epoch: 1, batch: 10260, loss: 0.07112, speed: 0.55 step/s\n",
      "global step 10270, epoch: 1, batch: 10270, loss: 0.07980, speed: 0.56 step/s\n",
      "global step 10280, epoch: 1, batch: 10280, loss: 0.07361, speed: 0.46 step/s\n",
      "global step 10290, epoch: 1, batch: 10290, loss: 0.06579, speed: 0.51 step/s\n",
      "global step 10300, epoch: 1, batch: 10300, loss: 0.08460, speed: 0.65 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 23:02:13,705] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10300\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 23:02:13,707] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10300\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10310, epoch: 1, batch: 10310, loss: 0.07262, speed: 0.01 step/s\n",
      "global step 10320, epoch: 1, batch: 10320, loss: 0.07561, speed: 0.62 step/s\n",
      "global step 10330, epoch: 1, batch: 10330, loss: 0.06772, speed: 0.49 step/s\n",
      "global step 10340, epoch: 1, batch: 10340, loss: 0.09071, speed: 0.51 step/s\n",
      "global step 10350, epoch: 1, batch: 10350, loss: 0.08074, speed: 0.50 step/s\n",
      "global step 10360, epoch: 1, batch: 10360, loss: 0.04587, speed: 0.50 step/s\n",
      "global step 10370, epoch: 1, batch: 10370, loss: 0.06067, speed: 0.52 step/s\n",
      "global step 10380, epoch: 1, batch: 10380, loss: 0.06348, speed: 0.52 step/s\n",
      "global step 10390, epoch: 1, batch: 10390, loss: 0.06067, speed: 0.49 step/s\n",
      "global step 10400, epoch: 1, batch: 10400, loss: 0.08130, speed: 0.52 step/s\n",
      "phase: dev, auc: 0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 23:14:28,634] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10400\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 23:14:28,637] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10400\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10410, epoch: 1, batch: 10410, loss: 0.06866, speed: 0.02 step/s\n",
      "global step 10420, epoch: 1, batch: 10420, loss: 0.07058, speed: 0.54 step/s\n",
      "global step 10430, epoch: 1, batch: 10430, loss: 0.08628, speed: 0.56 step/s\n",
      "global step 10440, epoch: 1, batch: 10440, loss: 0.07907, speed: 0.56 step/s\n",
      "global step 10450, epoch: 1, batch: 10450, loss: 0.07688, speed: 0.53 step/s\n",
      "global step 10460, epoch: 1, batch: 10460, loss: 0.08984, speed: 0.56 step/s\n",
      "global step 10470, epoch: 1, batch: 10470, loss: 0.05632, speed: 0.47 step/s\n",
      "global step 10480, epoch: 1, batch: 10480, loss: 0.08749, speed: 0.60 step/s\n",
      "global step 10490, epoch: 1, batch: 10490, loss: 0.06752, speed: 0.52 step/s\n",
      "global step 10500, epoch: 1, batch: 10500, loss: 0.06127, speed: 0.59 step/s\n",
      "phase: dev, auc: 0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 23:28:08,359] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10500\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 23:28:08,360] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10500\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10510, epoch: 1, batch: 10510, loss: 0.07983, speed: 0.02 step/s\n",
      "global step 10520, epoch: 1, batch: 10520, loss: 0.08310, speed: 0.56 step/s\n",
      "global step 10530, epoch: 1, batch: 10530, loss: 0.09677, speed: 0.51 step/s\n",
      "global step 10540, epoch: 1, batch: 10540, loss: 0.07972, speed: 0.51 step/s\n",
      "global step 10550, epoch: 1, batch: 10550, loss: 0.05092, speed: 0.58 step/s\n",
      "global step 10560, epoch: 1, batch: 10560, loss: 0.06852, speed: 0.55 step/s\n",
      "global step 10570, epoch: 1, batch: 10570, loss: 0.08939, speed: 0.47 step/s\n",
      "global step 10580, epoch: 1, batch: 10580, loss: 0.07445, speed: 0.51 step/s\n",
      "global step 10590, epoch: 1, batch: 10590, loss: 0.07644, speed: 0.58 step/s\n",
      "global step 10600, epoch: 1, batch: 10600, loss: 0.07175, speed: 0.51 step/s\n",
      "phase: dev, auc: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 23:41:21,628] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10600\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 23:41:21,630] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10600\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10610, epoch: 1, batch: 10610, loss: 0.06714, speed: 0.02 step/s\n",
      "global step 10620, epoch: 1, batch: 10620, loss: 0.07407, speed: 0.57 step/s\n",
      "global step 10630, epoch: 1, batch: 10630, loss: 0.06235, speed: 0.50 step/s\n",
      "global step 10640, epoch: 1, batch: 10640, loss: 0.07812, speed: 0.52 step/s\n",
      "global step 10650, epoch: 1, batch: 10650, loss: 0.08669, speed: 0.55 step/s\n",
      "global step 10660, epoch: 1, batch: 10660, loss: 0.05792, speed: 0.49 step/s\n",
      "global step 10670, epoch: 1, batch: 10670, loss: 0.05511, speed: 0.54 step/s\n",
      "global step 10680, epoch: 1, batch: 10680, loss: 0.04367, speed: 0.61 step/s\n",
      "global step 10690, epoch: 1, batch: 10690, loss: 0.08699, speed: 0.60 step/s\n",
      "global step 10700, epoch: 1, batch: 10700, loss: 0.10439, speed: 0.53 step/s\n",
      "phase: dev, auc: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-07 23:57:26,930] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10700\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-07 23:57:26,933] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10700\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10710, epoch: 1, batch: 10710, loss: 0.04751, speed: 0.01 step/s\n",
      "global step 10720, epoch: 1, batch: 10720, loss: 0.09038, speed: 0.56 step/s\n",
      "global step 10730, epoch: 1, batch: 10730, loss: 0.08591, speed: 0.50 step/s\n",
      "global step 10740, epoch: 1, batch: 10740, loss: 0.05576, speed: 0.58 step/s\n",
      "global step 10750, epoch: 1, batch: 10750, loss: 0.07238, speed: 0.50 step/s\n",
      "global step 10760, epoch: 1, batch: 10760, loss: 0.06476, speed: 0.49 step/s\n",
      "global step 10770, epoch: 1, batch: 10770, loss: 0.08743, speed: 0.56 step/s\n",
      "global step 10780, epoch: 1, batch: 10780, loss: 0.05192, speed: 0.52 step/s\n",
      "global step 10790, epoch: 1, batch: 10790, loss: 0.07540, speed: 0.55 step/s\n",
      "global step 10800, epoch: 1, batch: 10800, loss: 0.03917, speed: 0.47 step/s\n",
      "phase: dev, auc: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-08 00:08:31,636] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10800\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-08 00:08:31,638] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10800\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10810, epoch: 1, batch: 10810, loss: 0.07401, speed: 0.02 step/s\n",
      "global step 10820, epoch: 1, batch: 10820, loss: 0.09065, speed: 0.54 step/s\n",
      "global step 10830, epoch: 1, batch: 10830, loss: 0.07131, speed: 0.66 step/s\n",
      "global step 10840, epoch: 1, batch: 10840, loss: 0.05337, speed: 0.50 step/s\n",
      "global step 10850, epoch: 1, batch: 10850, loss: 0.07657, speed: 0.44 step/s\n",
      "global step 10860, epoch: 1, batch: 10860, loss: 0.06588, speed: 0.52 step/s\n",
      "global step 10870, epoch: 1, batch: 10870, loss: 0.06121, speed: 0.55 step/s\n",
      "global step 10880, epoch: 1, batch: 10880, loss: 0.07449, speed: 0.60 step/s\n",
      "global step 10890, epoch: 1, batch: 10890, loss: 0.10121, speed: 0.49 step/s\n",
      "global step 10900, epoch: 1, batch: 10900, loss: 0.09062, speed: 0.45 step/s\n",
      "phase: dev, auc: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-02-08 00:22:26,046] [    INFO]\u001b[0m - tokenizer config file saved in model_param\\model_10900\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2025-02-08 00:22:26,048] [    INFO]\u001b[0m - Special tokens file saved in model_param\\model_10900\\special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10910, epoch: 1, batch: 10910, loss: 0.05325, speed: 0.02 step/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal step \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, epoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, batch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m, speed: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m step/s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;241m%\u001b[39m (global_step, epoch, step, loss, \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m/\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tic_train)))\n\u001b[0;32m     23\u001b[0m     tic_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddle\\base\\wrapped_decorator.py:26\u001b[0m, in \u001b[0;36mwrap_decorator.<locals>.__impl__\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__impl__\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     25\u001b[0m     wrapped_func \u001b[38;5;241m=\u001b[39m decorator_func(func)\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddle\\base\\framework.py:593\u001b[0m, in \u001b[0;36m_dygraph_only_.<locals>.__impl__\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__impl__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_dygraph_mode(), (\n\u001b[0;32m    590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe only support \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in dynamic graph mode, please call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaddle.disable_static()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to enter dynamic graph mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    591\u001b[0m         \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    592\u001b[0m     )\n\u001b[1;32m--> 593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddle\\base\\dygraph\\tensor_patch_methods.py:342\u001b[0m, in \u001b[0;36mmonkey_patch_tensor.<locals>.backward\u001b[1;34m(self, grad_tensor, retain_graph)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _grad_scalar:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# When using amp with Fleet DistributedStrategy, we do loss scaling implicitly.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m _grad_scalar\u001b[38;5;241m.\u001b[39mscale(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 342\u001b[0m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_profiler_mode():\n\u001b[0;32m    345\u001b[0m     record_event\u001b[38;5;241m.\u001b[39mend()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 下面正式开始训练模型\n",
    "\n",
    "save_dir=\"model_param\"\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1): \n",
    "        pos_input_ids, pos_token_type_ids, neg_input_ids, neg_token_type_ids = batch\n",
    "\n",
    "        loss = model(\n",
    "            pos_input_ids=pos_input_ids,\n",
    "            neg_input_ids=neg_input_ids,\n",
    "            pos_token_type_ids=pos_token_type_ids,\n",
    "            neg_token_type_ids=neg_token_type_ids,\n",
    "        ) \n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % 10 == 0: \n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, 10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            evaluate(model, metric, dev_data_loader)\n",
    "            \n",
    "            save_path = os.path.join(save_dir, \"model_%d\" % global_step) \n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "\n",
    "            save_param_path = os.path.join(save_path, \"model_state.pdparams\") \n",
    "            paddle.save(model.state_dict(), save_param_path)\n",
    "            tokenizer.save_pretrained(save_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m paddle\u001b[38;5;241m.\u001b[39mload(params_path)\n\u001b[0;32m      8\u001b[0m model_2\u001b[38;5;241m.\u001b[39mset_dict(state_dict)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_data_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddle\\base\\dygraph\\base.py:352\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>._decorate_function\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_decorate_function\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 352\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, metric, data_loader)\u001b[0m\n\u001b[0;32m      8\u001b[0m pos_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(input_ids\u001b[38;5;241m=\u001b[39minput_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids) \n\u001b[0;32m      9\u001b[0m neg_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m pos_probs \n\u001b[1;32m---> 10\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_probs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     12\u001b[0m metric\u001b[38;5;241m.\u001b[39mupdate(preds\u001b[38;5;241m=\u001b[39mpreds, labels\u001b[38;5;241m=\u001b[39mlabels) \n\u001b[0;32m     13\u001b[0m auc \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39maccumulate() \n",
      "File \u001b[1;32mc:\\Users\\LJX\\Anaconda3\\envs\\search\\lib\\site-packages\\paddle\\base\\dygraph\\tensor_patch_methods.py:878\u001b[0m, in \u001b[0;36mmonkey_patch_tensor.<locals>.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__array__\u001b[39m(\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    853\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    854\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    855\u001b[0m ):\n\u001b[0;32m    856\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;124;03m    Returns a numpy array shows the value of current Tensor.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03m            (2, 2)\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype:\n\u001b[0;32m    880\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 下面是把保存好的模型参数文件加载到一个新的模型中，并进行性能测试的过程\n",
    "\n",
    "model_2 = PairwiseMatching(pretrained_model, margin=0.1)\n",
    "\n",
    "params_path = \"model_param/model_400/model_state.pdparams\"\n",
    "\n",
    "state_dict = paddle.load(params_path)\n",
    "model_2.set_dict(state_dict)\n",
    "\n",
    "evaluate(model_2, metric, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是把保存好的模型参数文件加载到一个新的模型中，并进行性能测试的过程\n",
    "\n",
    "model_2 = PairwiseMatching(pretrained_model, margin=0.1)\n",
    "\n",
    "params_path = \"model_param/model_10700/model_state.pdparams\"\n",
    "\n",
    "state_dict = paddle.load(params_path)\n",
    "model_2.set_dict(state_dict)\n",
    "\n",
    "evaluate(model_2, metric, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
